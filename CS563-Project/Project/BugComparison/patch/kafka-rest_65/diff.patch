diff --git a/README.md b/README.md
index 24661f7607..8c07b534ac 100644
--- a/README.md
+++ b/README.md
@@ -36,26 +36,34 @@ the REST Proxy running using the default settings and some topics already create
       {"value_schema_id":0,"offsets":[{"partition":0,"offset":0}]}
 
     # Create a consumer for binary data, starting at the beginning of the topic's
-    # log. Then consume some data from a topic.
+    # log. Then consume some data from a topic using the base URL in the first response.
+    # Finally, close the consumer with a DELETE to make it leave the group and clean up
+    # its resources.
     $ curl -X POST -H "Content-Type: application/vnd.kafka.v1+json" \
-          --data '{"id": "my_instance", "format": "binary", "auto.offset.reset": "smallest"}' \
+          --data '{"format": "binary", "auto.offset.reset": "smallest"}' \
           http://localhost:8082/consumers/my_binary_consumer
-      {"instance_id":"my_instance","base_uri":"http://localhost:8082/consumers/my_binary_consumer/instances/my_instance"}
+      {"instance_id":"rest-consumer-11561681-8ba5-4b46-bed0-905ae1769bc6","base_uri":"http://localhost:8082/consumers/my_binary_consumer/instances/rest-consumer-11561681-8ba5-4b46-bed0-905ae1769bc6"}
     $ curl -X GET -H "Accept: application/vnd.kafka.binary.v1+json" \
-          http://localhost:8082/consumers/my_binary_consumer/instances/my_instance/topics/test
-      [{"value":"S2Fma2E=","partition":0,"offset":0},{"value":"S2Fma2E=","partition":0,"offset":1}]
+          http://localhost:8082/consumers/my_binary_consumer/instances/rest-consumer-11561681-8ba5-4b46-bed0-905ae1769bc6/topics/test
+      [{"key":null,"value":"S2Fma2E=","partition":0,"offset":0}]
+    $ curl -X DELETE \
+          http://localhost:8082/consumers/my_binary_consumer/instances/rest-consumer-11561681-8ba5-4b46-bed0-905ae1769bc6
+      # No content in response
 
     # Create a consumer for Avro data, starting at the beginning of the topic's
     # log. Then consume some data from a topic, which is decoded, translated to
     # JSON, and included in the response. The schema used for deserialization is
-    # fetched automatically from the schema registry.
+    # fetched automatically from the schema registry. Finally, clean up.
     $ curl -X POST -H "Content-Type: application/vnd.kafka.v1+json" \
-          --data '{"id": "my_instance", "format": "avro", "auto.offset.reset": "smallest"}' \
+          --data '{"format": "avro", "auto.offset.reset": "smallest"}' \
           http://localhost:8082/consumers/my_avro_consumer
-      {"instance_id":"my_instance","base_uri":"http://localhost:8082/consumers/my_avro_consumer/instances/my_instance"}
+      {"instance_id":"rest-consumer-11392f3a-efbe-4fe2-b0bf-5c85d7b25e7b","base_uri":"http://localhost:8082/consumers/my_avro_consumer/instances/rest-consumer-11392f3a-efbe-4fe2-b0bf-5c85d7b25e7b"}
     $ curl -X GET -H "Accept: application/vnd.kafka.avro.v1+json" \
-          http://localhost:8082/consumers/my_avro_consumer/instances/my_instance/topics/avrotest
-      [{"value":{"name":"testUser"},"partition":0,"offset":0},{"value":{"name":"testUser2"},"partition":0,"offset":1}]
+          http://localhost:8082/consumers/my_avro_consumer/instances/rest-consumer-11392f3a-efbe-4fe2-b0bf-5c85d7b25e7b/topics/avrotest
+      [{"key":null,"value":{"name":"testUser"},"partition":0,"offset":0}]
+    $ curl -X DELETE \
+          http://localhost:8082/consumers/my_avro_consumer/instances/rest-consumer-11392f3a-efbe-4fe2-b0bf-5c85d7b25e7b
+      # No content in response
 
 Installation
 ------------
diff --git a/docs/api.rst b/docs/api.rst
index 9aae7d24af..37bbb40fab 100644
--- a/docs/api.rst
+++ b/docs/api.rst
@@ -656,8 +656,12 @@ error.
    for this specific REST proxy instance.
 
    :param string group_name: The name of the consumer group to join
-   :<json string id: Unique ID for the consumer instance in this group. If omitted, one will be automatically generated
-                     using the REST proxy ID and an auto-incrementing number
+   :<json string id: **DEPRECATED** Unique ID for the consumer instance in this group. If omitted,
+                     one will be automatically generated
+   :<json string name: Name for the consumer instance, which will be used in URLs for the
+                       consumer. This must be unique, at least within the proxy process handling
+                       the request. If omitted, falls back on the automatically generated ID. Using
+                       automatically generated names is recommended for most use cases.
    :<json string format: The format of consumed messages, which is used to convert messages into
                          a JSON-compatible form. Valid values: "binary", "avro". If unspecified,
                          defaults to "binary".
@@ -669,6 +673,8 @@ error.
    :>json string base_uri: Base URI used to construct URIs for subsequent requests against this consumer instance. This
                            will be of the form ``http://hostname:port/consumers/consumer_group/instances/instance_id``.
 
+   :statuscode 409:
+          * Error code 40902 -- Consumer instance with the specified name already exists.
    :statuscode 422:
           * Error code 42204 -- Invalid consumer configuration. One of the settings specified in
             the request contained an invalid value.
@@ -682,7 +688,7 @@ error.
       Accept: application/vnd.kafka.v1+json, application/vnd.kafka+json, application/json
 
       {
-        "id": "my_consumer",
+        "name": "my_consumer",
         "format": "binary",
         "auto.offset.reset": "smallest",
         "auto.commit.enable": "false"
diff --git a/docs/intro.rst b/docs/intro.rst
index 80954c24d2..756f49813b 100644
--- a/docs/intro.rst
+++ b/docs/intro.rst
@@ -40,26 +40,34 @@ the REST Proxy running using the default settings and some topics already create
      {"offsets":[{"partition":0,"offset":0,"error_code":null,"error":null}],"key_schema_id":null,"value_schema_id":21}
 
    # Create a consumer for binary data, starting at the beginning of the topic's
-   # log. Then consume some data from a topic.
+   # log. Then consume some data from a topic using the base URL in the first response.
+   # Finally, close the consumer with a DELETE to make it leave the group and clean up
+   # its resources.
    $ curl -X POST -H "Content-Type: application/vnd.kafka.v1+json" \
-         --data '{"id": "my_instance", "format": "binary", "auto.offset.reset": "smallest"}' \
+         --data '{"format": "binary", "auto.offset.reset": "smallest"}' \
          http://localhost:8082/consumers/my_binary_consumer
-     {"instance_id":"my_instance","base_uri":"http://localhost:8082/consumers/my_binary_consumer/instances/my_instance"}
+     {"instance_id":"rest-consumer-11561681-8ba5-4b46-bed0-905ae1769bc6","base_uri":"http://localhost:8082/consumers/my_binary_consumer/instances/rest-consumer-11561681-8ba5-4b46-bed0-905ae1769bc6"}
    $ curl -X GET -H "Accept: application/vnd.kafka.binary.v1+json" \
-         http://localhost:8082/consumers/my_binary_consumer/instances/my_instance/topics/test
+         http://localhost:8082/consumers/my_binary_consumer/instances/rest-consumer-11561681-8ba5-4b46-bed0-905ae1769bc6/topics/test
      [{"key":null,"value":"S2Fma2E=","partition":0,"offset":0}]
+   $ curl -X DELETE \
+         http://localhost:8082/consumers/my_binary_consumer/instances/rest-consumer-11561681-8ba5-4b46-bed0-905ae1769bc6
+     # No content in response
 
    # Create a consumer for Avro data, starting at the beginning of the topic's
    # log. Then consume some data from a topic, which is decoded, translated to
    # JSON, and included in the response. The schema used for deserialization is
-   # fetched automatically from the schema registry.
+   # fetched automatically from the schema registry. Finally, clean up.
    $ curl -X POST -H "Content-Type: application/vnd.kafka.v1+json" \
-         --data '{"id": "my_instance", "format": "avro", "auto.offset.reset": "smallest"}' \
+         --data '{"format": "avro", "auto.offset.reset": "smallest"}' \
          http://localhost:8082/consumers/my_avro_consumer
-     {"instance_id":"my_instance","base_uri":"http://localhost:8082/consumers/my_avro_consumer/instances/my_instance"}
+     {"instance_id":"rest-consumer-11392f3a-efbe-4fe2-b0bf-5c85d7b25e7b","base_uri":"http://localhost:8082/consumers/my_avro_consumer/instances/rest-consumer-11392f3a-efbe-4fe2-b0bf-5c85d7b25e7b"}
    $ curl -X GET -H "Accept: application/vnd.kafka.avro.v1+json" \
-         http://localhost:8082/consumers/my_avro_consumer/instances/my_instance/topics/avrotest
+         http://localhost:8082/consumers/my_avro_consumer/instances/rest-consumer-11392f3a-efbe-4fe2-b0bf-5c85d7b25e7b/topics/avrotest
      [{"key":null,"value":{"name":"testUser"},"partition":0,"offset":0}]
+   $ curl -X DELETE \
+         http://localhost:8082/consumers/my_avro_consumer/instances/rest-consumer-11392f3a-efbe-4fe2-b0bf-5c85d7b25e7b
+     # No content in response
 
 
 Features
diff --git a/src/main/java/io/confluent/kafkarest/ConsumerManager.java b/src/main/java/io/confluent/kafkarest/ConsumerManager.java
index ecd02adcef..69ece3f9c7 100644
--- a/src/main/java/io/confluent/kafkarest/ConsumerManager.java
+++ b/src/main/java/io/confluent/kafkarest/ConsumerManager.java
@@ -23,6 +23,7 @@
 import java.util.Map;
 import java.util.PriorityQueue;
 import java.util.Properties;
+import java.util.UUID;
 import java.util.Vector;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.ExecutorService;
@@ -58,7 +59,6 @@ public class ConsumerManager {
   private final MetadataObserver mdObserver;
   private final int iteratorTimeoutMs;
 
-  private final AtomicInteger nextId = new AtomicInteger(0);
   // ConsumerState is generic, but we store them untyped here. This allows many operations to
   // work without having to know the types for the consumer, only requiring type information
   // during read operations.
@@ -110,51 +110,76 @@ public ConsumerManager(KafkaRestConfig config, MetadataObserver mdObserver,
    * @return Unique consumer instance ID
    */
   public String createConsumer(String group, ConsumerInstanceConfig instanceConfig) {
-    String id = instanceConfig.getId();
-    if (id == null) {
-      id = "rest-consumer-";
+    // The terminology here got mixed up for historical reasons, and remaining compatible moving
+    // forward is tricky. To maintain compatibility, if the 'id' field is specified we maintain
+    // the previous behavior of using it's value in both the URLs for the consumer (i.e. the
+    // local name) and the ID (consumer.id setting in the consumer). Otherwise, the 'name' field
+    // only applies to the local name. When we replace with the new consumer, we may want to
+    // provide an alternate app name, or just reuse the name.
+    String name = instanceConfig.getName();
+    if (instanceConfig.getId() != null) { // Explicit ID request always overrides name
+      name = instanceConfig.getId();
+    }
+    if (name == null) {
+      name = "rest-consumer-";
       String serverId = this.config.getString(KafkaRestConfig.ID_CONFIG);
       if (!serverId.isEmpty()) {
-        id += serverId + "-";
+        name += serverId + "-";
       }
-      id += ((Integer) nextId.incrementAndGet()).toString();
+      name += UUID.randomUUID().toString();
     }
 
-    log.debug("Creating consumer " + id + " in group " + group);
-
-    // Note the ordering here. We want to allow overrides, but almost all the
-    // consumer-specific settings don't make sense to override globally (e.g. group ID, consumer
-    // ID), and others we want to ensure get overridden (e.g. consumer.timeout.ms, which we
-    // intentionally name differently in our own configs).
-    Properties props = (Properties) config.getOriginalProperties().clone();
-    props.setProperty("zookeeper.connect", zookeeperConnect);
-    props.setProperty("group.id", group);
-    props.setProperty("consumer.id", id);
-    // To support the old consumer interface with broken peek()/missing poll(timeout)
-    // functionality, we always use a timeout. This can't perfectly guarantee a total request
-    // timeout, but can get as close as this timeout's value
-    props.setProperty("consumer.timeout.ms", ((Integer) iteratorTimeoutMs).toString());
-    if (instanceConfig.getAutoCommitEnable() != null) {
-      props.setProperty("auto.commit.enable", instanceConfig.getAutoCommitEnable());
-    } else {
-      props.setProperty("auto.commit.enable", "false");
-    }
-    if (instanceConfig.getAutoOffsetReset() != null) {
-      props.setProperty("auto.offset.reset", instanceConfig.getAutoOffsetReset());
+    ConsumerInstanceId cid = new ConsumerInstanceId(group, name);
+    // Perform this check before
+    synchronized (this) {
+      if (consumers.containsKey(cid)) {
+        throw Errors.consumerAlreadyExistsException();
+      } else {
+        // Placeholder to reserve this ID
+        consumers.put(cid, null);
+      }
     }
-    ConsumerConnector consumer;
+
+    // Ensure we clean up the placeholder if there are any issues creating the consumer instance
+    boolean succeeded = false;
     try {
-      if (consumerFactory == null) {
-        consumer = Consumer.createJavaConsumerConnector(new ConsumerConfig(props));
+      log.debug("Creating consumer " + name + " in group " + group);
+
+      // Note the ordering here. We want to allow overrides, but almost all the
+      // consumer-specific settings don't make sense to override globally (e.g. group ID, consumer
+      // ID), and others we want to ensure get overridden (e.g. consumer.timeout.ms, which we
+      // intentionally name differently in our own configs).
+      Properties props = (Properties) config.getOriginalProperties().clone();
+      props.setProperty("zookeeper.connect", zookeeperConnect);
+      props.setProperty("group.id", group);
+      // This ID we pass here has to be unique, only pass a value along if the deprecated ID field
+      // was passed in. This generally shouldn't be used, but is maintained for compatibility.
+      if (instanceConfig.getId() != null) {
+        props.setProperty("consumer.id", instanceConfig.getId());
+      }
+      // To support the old consumer interface with broken peek()/missing poll(timeout)
+      // functionality, we always use a timeout. This can't perfectly guarantee a total request
+      // timeout, but can get as close as this timeout's value
+      props.setProperty("consumer.timeout.ms", ((Integer) iteratorTimeoutMs).toString());
+      if (instanceConfig.getAutoCommitEnable() != null) {
+        props.setProperty("auto.commit.enable", instanceConfig.getAutoCommitEnable());
       } else {
-        consumer = consumerFactory.createConsumer(new ConsumerConfig(props));
+        props.setProperty("auto.commit.enable", "false");
+      }
+      if (instanceConfig.getAutoOffsetReset() != null) {
+        props.setProperty("auto.offset.reset", instanceConfig.getAutoOffsetReset());
+      }
+      ConsumerConnector consumer;
+      try {
+        if (consumerFactory == null) {
+          consumer = Consumer.createJavaConsumerConnector(new ConsumerConfig(props));
+        } else {
+          consumer = consumerFactory.createConsumer(new ConsumerConfig(props));
+        }
+      } catch (InvalidConfigException e) {
+        throw Errors.invalidConsumerConfigException(e);
       }
-    } catch (InvalidConfigException e) {
-      throw Errors.invalidConsumerConfigException(e);
-    }
 
-    synchronized (this) {
-      ConsumerInstanceId cid = new ConsumerInstanceId(group, id);
       ConsumerState state;
       switch (instanceConfig.getFormat()) {
         case BINARY:
@@ -168,14 +193,21 @@ public String createConsumer(String group, ConsumerInstanceConfig instanceConfig
                                              Response.Status.INTERNAL_SERVER_ERROR.getStatusCode());
       }
 
-      consumers.put(cid, state);
-      consumersByExpiration.add(state);
-      this.notifyAll();
+      synchronized (this) {
+        consumers.put(cid, state);
+        consumersByExpiration.add(state);
+        this.notifyAll();
+      }
+      succeeded = true;
+      return name;
+    } finally {
+      if (!succeeded) {
+        synchronized (this) {
+          consumers.remove(cid);
+        }
+      }
     }
-
-    return id;
   }
-
   public interface ReadCallback<K, V> {
 
     public void onCompletion(List<? extends ConsumerRecord<K, V>> records, Exception e);
diff --git a/src/main/java/io/confluent/kafkarest/Errors.java b/src/main/java/io/confluent/kafkarest/Errors.java
index cf81a6a920..53fdaacb13 100644
--- a/src/main/java/io/confluent/kafkarest/Errors.java
+++ b/src/main/java/io/confluent/kafkarest/Errors.java
@@ -75,6 +75,16 @@ public static RestException consumerAlreadySubscribedException() {
                              CONSUMER_ALREADY_SUBSCRIBED_ERROR_CODE);
   }
 
+  public final static String CONSUMER_ALREADY_EXISTS_MESSAGE =
+      "Consumer with specified consumer ID already exists in the specified consumer group.";
+  public final static int CONSUMER_ALREADY_EXISTS_ERROR_CODE = 40902;
+
+  public static RestException consumerAlreadyExistsException() {
+    return new RestException(CONSUMER_ALREADY_EXISTS_MESSAGE,
+                             Response.Status.CONFLICT.getStatusCode(),
+                             CONSUMER_ALREADY_EXISTS_ERROR_CODE);
+  }
+
 
   public final static String KEY_SCHEMA_MISSING_MESSAGE = "Request includes keys but does not "
                                                           + "include key schema";
diff --git a/src/main/java/io/confluent/kafkarest/entities/ConsumerInstanceConfig.java b/src/main/java/io/confluent/kafkarest/entities/ConsumerInstanceConfig.java
index 7066496e5b..5178e8ff1d 100644
--- a/src/main/java/io/confluent/kafkarest/entities/ConsumerInstanceConfig.java
+++ b/src/main/java/io/confluent/kafkarest/entities/ConsumerInstanceConfig.java
@@ -27,6 +27,7 @@ public class ConsumerInstanceConfig {
   private static final EmbeddedFormat DEFAULT_FORMAT = EmbeddedFormat.BINARY;
 
   private String id;
+  private String name;
   @NotNull
   private EmbeddedFormat format;
   private String autoOffsetReset;
@@ -38,14 +39,16 @@ public ConsumerInstanceConfig() {
 
   public ConsumerInstanceConfig(EmbeddedFormat format) {
     // This constructor is only for tests so reparsing the format name is ok
-    this(null, format.name(), null, null);
+    this(null, null, format.name(), null, null);
   }
 
   public ConsumerInstanceConfig(@JsonProperty("id") String id,
+                                @JsonProperty("name") String name,
                                 @JsonProperty("format") String format,
                                 @JsonProperty("auto.offset.reset") String autoOffsetReset,
                                 @JsonProperty("auto.commit.enable") String autoCommitEnable) {
     this.id = id;
+    this.name = name;
     if (format == null) {
       this.format = DEFAULT_FORMAT;
     } else {
@@ -76,6 +79,16 @@ public void setId(String id) {
     this.id = id;
   }
 
+  @JsonProperty
+  public String getName() {
+    return name;
+  }
+
+  @JsonProperty
+  public void setName(String name) {
+    this.name = name;
+  }
+
   @JsonIgnore
   public EmbeddedFormat getFormat() {
     return format;
diff --git a/src/test/java/io/confluent/kafkarest/integration/AbstractConsumerTest.java b/src/test/java/io/confluent/kafkarest/integration/AbstractConsumerTest.java
index 275c480dfc..eb205ad3b3 100644
--- a/src/test/java/io/confluent/kafkarest/integration/AbstractConsumerTest.java
+++ b/src/test/java/io/confluent/kafkarest/integration/AbstractConsumerTest.java
@@ -21,6 +21,8 @@
 import org.apache.kafka.clients.producer.ProducerRecord;
 import org.apache.kafka.common.serialization.ByteArraySerializer;
 
+import java.net.MalformedURLException;
+import java.net.URL;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -100,11 +102,31 @@ protected void produceAvroMessages(List<ProducerRecord<Object, Object>> records)
     producer.close();
   }
 
+  protected Response createConsumerInstance(String groupName, String id,
+                                            String name, EmbeddedFormat format) {
+    ConsumerInstanceConfig config = null;
+    if (id != null || name != null || format != null) {
+      config = new ConsumerInstanceConfig(
+          id, name, (format != null ? format.toString() : null), null, null);
+    }
+    return request("/consumers/" + groupName)
+        .post(Entity.entity(config, Versions.KAFKA_MOST_SPECIFIC_DEFAULT));
+  }
+
+  protected String consumerNameFromInstanceUrl(String url) {
+    try {
+      String[] pathComponents = new URL(url).getPath().split("/");
+      return pathComponents[pathComponents.length-1];
+    } catch (MalformedURLException e) {
+      throw new RuntimeException(e);
+    }
+  }
+
   // Need to start consuming before producing since consumer is instantiated internally and
   // starts at latest offset
   protected String startConsumeMessages(String groupName, String topic, EmbeddedFormat format,
-                                        String accept, String expectedMediatype) {
-    return startConsumeMessages(groupName, topic, format, accept, expectedMediatype, false);
+                                        String expectedMediatype) {
+    return startConsumeMessages(groupName, topic, format, expectedMediatype, false);
   }
 
   /**
@@ -115,21 +137,18 @@ protected String startConsumeMessages(String groupName, String topic, EmbeddedFo
    * @param topic             topic to consume
    * @param format            embedded format to use. If null, an null ConsumerInstanceConfig is
    *                          sent, resulting in default settings
-   * @param accept            mediatype for Accept header, or null to omit the header
    * @param expectedMediatype expected Content-Type of response
    * @param expectFailure     if true, expect the initial read request to generate a 404
    * @return the new consumer instance's base URI
    */
   protected String startConsumeMessages(String groupName, String topic, EmbeddedFormat format,
-                                        String accept, String expectedMediatype,
+                                        String expectedMediatype,
                                         boolean expectFailure) {
-    ConsumerInstanceConfig config = null;
-    if (format != null) {
-      config = new ConsumerInstanceConfig(format);
-    }
-    CreateConsumerInstanceResponse instanceResponse = request("/consumers/" + groupName)
-        .post(Entity.entity(config, Versions.KAFKA_MOST_SPECIFIC_DEFAULT),
-              CreateConsumerInstanceResponse.class);
+    Response createResponse = createConsumerInstance(groupName, null, null, format);
+    assertOKResponse(createResponse, Versions.KAFKA_MOST_SPECIFIC_DEFAULT);
+
+    CreateConsumerInstanceResponse instanceResponse =
+        createResponse.readEntity(CreateConsumerInstanceResponse.class);
     assertNotNull(instanceResponse.getInstanceId());
     assertTrue(instanceResponse.getInstanceId().length() > 0);
     assertTrue("Base URI should contain the consumer instance ID",
diff --git a/src/test/java/io/confluent/kafkarest/integration/ConsumerAvroTest.java b/src/test/java/io/confluent/kafkarest/integration/ConsumerAvroTest.java
index 6b84489bc0..e21e78ae2b 100644
--- a/src/test/java/io/confluent/kafkarest/integration/ConsumerAvroTest.java
+++ b/src/test/java/io/confluent/kafkarest/integration/ConsumerAvroTest.java
@@ -107,7 +107,6 @@ public void setUp() throws Exception {
   @Test
   public void testConsumeOnlyValues() {
     String instanceUri = startConsumeMessages(groupName, topicName, EmbeddedFormat.AVRO,
-                                              Versions.KAFKA_V1_JSON_AVRO,
                                               Versions.KAFKA_V1_JSON_AVRO);
     produceAvroMessages(recordsOnlyValues);
     consumeMessages(instanceUri, topicName, recordsOnlyValues,
@@ -119,7 +118,6 @@ public void testConsumeOnlyValues() {
   @Test
   public void testConsumeWithKeys() {
     String instanceUri = startConsumeMessages(groupName, topicName, EmbeddedFormat.AVRO,
-                                              Versions.KAFKA_V1_JSON_AVRO,
                                               Versions.KAFKA_V1_JSON_AVRO);
     produceAvroMessages(recordsWithKeys);
     consumeMessages(instanceUri, topicName, recordsWithKeys,
@@ -131,13 +129,12 @@ public void testConsumeWithKeys() {
   @Test
   public void testConsumeInvalidTopic() {
     startConsumeMessages(groupName, "nonexistenttopic", EmbeddedFormat.AVRO,
-                         Versions.KAFKA_V1_JSON_AVRO, Versions.KAFKA_V1_JSON_AVRO, true);
+                         Versions.KAFKA_V1_JSON_AVRO, true);
   }
 
   @Test
   public void testConsumeTimeout() {
     String instanceUri = startConsumeMessages(groupName, topicName, EmbeddedFormat.AVRO,
-                                              Versions.KAFKA_V1_JSON_AVRO,
                                               Versions.KAFKA_V1_JSON_AVRO);
     produceAvroMessages(recordsWithKeys);
     consumeMessages(instanceUri, topicName, recordsWithKeys,
@@ -151,7 +148,6 @@ public void testConsumeTimeout() {
   @Test
   public void testDeleteConsumer() {
     String instanceUri = startConsumeMessages(groupName, topicName, EmbeddedFormat.AVRO,
-                                              Versions.KAFKA_V1_JSON_AVRO,
                                               Versions.KAFKA_V1_JSON_AVRO);
     produceAvroMessages(recordsWithKeys);
     consumeMessages(instanceUri, topicName, recordsWithKeys,
diff --git a/src/test/java/io/confluent/kafkarest/integration/ConsumerBinaryTest.java b/src/test/java/io/confluent/kafkarest/integration/ConsumerBinaryTest.java
index dcb7030cba..de9d170900 100644
--- a/src/test/java/io/confluent/kafkarest/integration/ConsumerBinaryTest.java
+++ b/src/test/java/io/confluent/kafkarest/integration/ConsumerBinaryTest.java
@@ -86,7 +86,7 @@ public void testConsumeOnlyValues() {
     // Between these tests we either leave the config null or request the binary embedded format
     // so we can test that both will result in binary consumers. We also us varying accept
     // parameters to test that we default to Binary for various values.
-    String instanceUri = startConsumeMessages(groupName, topicName, null, null,
+    String instanceUri = startConsumeMessages(groupName, topicName, null,
                                               Versions.KAFKA_V1_JSON_BINARY);
     produceBinaryMessages(recordsOnlyValues);
     consumeMessages(instanceUri, topicName, recordsOnlyValues,
@@ -98,7 +98,6 @@ public void testConsumeOnlyValues() {
   @Test
   public void testConsumeWithKeys() {
     String instanceUri = startConsumeMessages(groupName, topicName, EmbeddedFormat.BINARY,
-                                              Versions.KAFKA_V1_JSON_BINARY,
                                               Versions.KAFKA_V1_JSON_BINARY);
     produceBinaryMessages(recordsWithKeys);
     consumeMessages(instanceUri, topicName, recordsWithKeys,
@@ -110,13 +109,12 @@ public void testConsumeWithKeys() {
   @Test
   public void testConsumeInvalidTopic() {
     startConsumeMessages(groupName, "nonexistenttopic", null,
-                         Versions.KAFKA_V1_JSON_BINARY, Versions.KAFKA_V1_JSON_BINARY, true);
+                         Versions.KAFKA_V1_JSON_BINARY, true);
   }
 
   @Test
   public void testConsumeTimeout() {
     String instanceUri = startConsumeMessages(groupName, topicName, EmbeddedFormat.BINARY,
-                                              Versions.KAFKA_V1_JSON,
                                               Versions.KAFKA_V1_JSON_BINARY);
     produceBinaryMessages(recordsWithKeys);
     consumeMessages(instanceUri, topicName, recordsWithKeys,
@@ -131,7 +129,6 @@ public void testConsumeTimeout() {
   @Test
   public void testDeleteConsumer() {
     String instanceUri = startConsumeMessages(groupName, topicName, null,
-                                              Versions.KAFKA_DEFAULT_JSON,
                                               Versions.KAFKA_V1_JSON_BINARY);
     produceBinaryMessages(recordsWithKeys);
     consumeMessages(instanceUri, topicName, recordsWithKeys,
@@ -145,7 +142,8 @@ public void testDeleteConsumer() {
   // that isn't specific to the type of embedded data, but since they need
   @Test
   public void testInvalidKafkaConsumerConfig() {
-    ConsumerInstanceConfig config = new ConsumerInstanceConfig("id", "binary", "bad-config", null);
+    ConsumerInstanceConfig config = new ConsumerInstanceConfig("id", "name", "binary",
+                                                               "bad-config", null);
     Response response = request("/consumers/" + groupName)
         .post(Entity.entity(config, Versions.KAFKA_V1_JSON));
     assertErrorResponse(ConstraintViolationExceptionMapper.UNPROCESSABLE_ENTITY, response,
@@ -153,4 +151,20 @@ public void testInvalidKafkaConsumerConfig() {
                         Errors.INVALID_CONSUMER_CONFIG_MESSAGE,
                         Versions.KAFKA_V1_JSON);
   }
+
+
+  @Test
+  public void testDuplicateConsumerID() {
+    String instanceUrl = startConsumeMessages(groupName, topicName, null,
+                                              Versions.KAFKA_V1_JSON_BINARY);
+    produceBinaryMessages(recordsWithKeys);
+
+    // Duplicate the same instance, which should cause a conflict
+    String name = consumerNameFromInstanceUrl(instanceUrl);
+    Response createResponse = createConsumerInstance(groupName, null, name, null);
+    assertErrorResponse(Response.Status.CONFLICT, createResponse,
+                        Errors.CONSUMER_ALREADY_EXISTS_ERROR_CODE,
+                        Errors.CONSUMER_ALREADY_EXISTS_MESSAGE,
+                        Versions.KAFKA_MOST_SPECIFIC_DEFAULT);
+  }
 }
diff --git a/src/test/java/io/confluent/kafkarest/integration/ConsumerTimeoutTest.java b/src/test/java/io/confluent/kafkarest/integration/ConsumerTimeoutTest.java
index 06695b3a88..71727cb57e 100644
--- a/src/test/java/io/confluent/kafkarest/integration/ConsumerTimeoutTest.java
+++ b/src/test/java/io/confluent/kafkarest/integration/ConsumerTimeoutTest.java
@@ -55,7 +55,6 @@ public void setUp() throws Exception {
   @Test
   public void testConsumerTimeout() throws InterruptedException {
     String instanceUri = startConsumeMessages(groupName, topicName, EmbeddedFormat.BINARY,
-                                              Versions.KAFKA_V1_JSON_BINARY,
                                               Versions.KAFKA_V1_JSON_BINARY);
     // Even with identical timeouts, should be able to consume multiple times without the
     // instance timing out
diff --git a/src/test/java/io/confluent/kafkarest/unit/ConsumerManagerTest.java b/src/test/java/io/confluent/kafkarest/unit/ConsumerManagerTest.java
index 9e6777b6da..1a09bf4e9b 100644
--- a/src/test/java/io/confluent/kafkarest/unit/ConsumerManagerTest.java
+++ b/src/test/java/io/confluent/kafkarest/unit/ConsumerManagerTest.java
@@ -53,6 +53,7 @@
 import static org.junit.Assert.assertNull;
 import static org.junit.Assert.assertThat;
 import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
 
 /**
  * Tests basic create/read/commit/delete functionality of ConsumerManager. This only exercises the
@@ -71,6 +72,8 @@ public class ConsumerManagerTest {
 
   private boolean sawCallback = false;
 
+  private Capture<ConsumerConfig> capturedConsumerConfig;
+
   @Before
   public void setUp() throws RestConfigException {
     Properties props = new Properties();
@@ -86,23 +89,40 @@ public void setUp() throws RestConfigException {
 
   private ConsumerConnector expectCreate(
       Map<String, List<Map<Integer, List<ConsumerRecord<byte[], byte[]>>>>> schedules) {
-    return expectCreate(schedules, false);
+    return expectCreate(schedules, false, null);
   }
 
   private ConsumerConnector expectCreate(
       Map<String, List<Map<Integer, List<ConsumerRecord<byte[], byte[]>>>>> schedules,
-      boolean allowMissingSchedule) {
+      boolean allowMissingSchedule, String requestedId) {
     ConsumerConnector
         consumer =
         new MockConsumerConnector(
             config.getTime(), "testclient", schedules,
             Integer.parseInt(KafkaRestConfig.CONSUMER_ITERATOR_TIMEOUT_MS_DEFAULT),
             allowMissingSchedule);
-    EasyMock.expect(consumerFactory.createConsumer(EasyMock.<ConsumerConfig>anyObject()))
-        .andReturn(consumer);
+    capturedConsumerConfig = new Capture<ConsumerConfig>();
+    EasyMock.expect(consumerFactory.createConsumer(EasyMock.capture(capturedConsumerConfig)))
+                        .andReturn(consumer);
     return consumer;
   }
 
+  // Expect a Kafka consumer to be created, but return it with no data in its queue. Used to test
+  // functionality that doesn't rely on actually consuming the data.
+  private ConsumerConnector expectCreateNoData(String requestedId) {
+    Map<Integer, List<ConsumerRecord<byte[], byte[]>>> referenceSchedule
+        = new HashMap<Integer, List<ConsumerRecord<byte[], byte[]>>>();
+    Map<String, List<Map<Integer, List<ConsumerRecord<byte[], byte[]>>>>> schedules
+        = new HashMap<String, List<Map<Integer, List<ConsumerRecord<byte[], byte[]>>>>>();
+    schedules.put(topicName, Arrays.asList(referenceSchedule));
+
+    return expectCreate(schedules, true, requestedId);
+  }
+
+  private ConsumerConnector expectCreateNoData() {
+    return expectCreateNoData(null);
+  }
+
   @Test
   public void testConsumerOverrides() {
     ConsumerConnector consumer =
@@ -254,14 +274,48 @@ public void onCompletion(List<? extends ConsumerRecord<byte[], byte[]>> records,
   }
 
   @Test
-  public void testMultipleTopicSubscriptionsFail() throws InterruptedException, ExecutionException {
-    Map<Integer, List<ConsumerRecord<byte[], byte[]>>> referenceSchedule
-        = new HashMap<Integer, List<ConsumerRecord<byte[], byte[]>>>();
-    Map<String, List<Map<Integer, List<ConsumerRecord<byte[], byte[]>>>>> schedules
-        = new HashMap<String, List<Map<Integer, List<ConsumerRecord<byte[], byte[]>>>>>();
-    schedules.put(topicName, Arrays.asList(referenceSchedule));
+  public void testIDOverridesName() {
+    // We should remain compatible with the original use of consumer IDs, even if it shouldn't
+    // really be used. Specifying any ID should override any naming to ensure the same behavior
+    expectCreateNoData("id");
+    EasyMock.replay(mdObserver, consumerFactory);
+
+    String cid = consumerManager.createConsumer(
+        groupName,
+        new ConsumerInstanceConfig("id", "name", EmbeddedFormat.BINARY.toString(), null, null)
+    );
+    assertEquals("id", cid);
+    assertEquals("id", capturedConsumerConfig.getValue().consumerId().getOrElse(null));
+    EasyMock.verify(mdObserver, consumerFactory);
+  }
+
+  @Test
+  public void testDuplicateConsumerName() {
+    expectCreateNoData();
+    EasyMock.replay(mdObserver, consumerFactory);
 
-    expectCreate(schedules, true);
+    consumerManager.createConsumer(
+        groupName,
+        new ConsumerInstanceConfig(null, "name", EmbeddedFormat.BINARY.toString(), null, null)
+    );
+
+    try {
+      consumerManager.createConsumer(
+          groupName,
+          new ConsumerInstanceConfig(null, "name", EmbeddedFormat.BINARY.toString(), null, null)
+      );
+      fail("Expected to see exception because consumer already exists");
+    } catch (RestException e) {
+      // expected
+      assertEquals(Errors.CONSUMER_ALREADY_EXISTS_ERROR_CODE, e.getErrorCode());
+    }
+
+    EasyMock.verify(mdObserver, consumerFactory);
+  }
+
+  @Test
+  public void testMultipleTopicSubscriptionsFail() throws InterruptedException, ExecutionException {
+    expectCreateNoData();
     EasyMock.expect(mdObserver.topicExists(topicName)).andReturn(true);
     EasyMock.expect(mdObserver.topicExists(secondTopicName)).andReturn(true);
     EasyMock.replay(mdObserver, consumerFactory);
