diff --git a/.idea/checkstyle-idea.xml b/.idea/checkstyle-idea.xml
index a716e20f..ee371e32 100644
--- a/.idea/checkstyle-idea.xml
+++ b/.idea/checkstyle-idea.xml
@@ -3,12 +3,12 @@
   <component name="CheckStyle-IDEA">
     <option name="configuration">
       <map>
-        <entry key="active-configuration" value="HTTP_URL:https://gitlab.gistools.geog.uni-heidelberg.de/giscience/big-data/ohsome/parent/raw/2.13.0/ohsome-codestyle/src/main/resources/checkstyle-google-ohsome.xml:ohsome Google Checks" />
-        <entry key="checkstyle-version" value="8.44" />
+        <entry key="active-configuration" value="HTTP_URL:https://gitlab.gistools.geog.uni-heidelberg.de/giscience/big-data/ohsome/parent/raw/2.14.0/ohsome-codestyle/src/main/resources/checkstyle-google-ohsome.xml:ohsome Google Checks" />
+        <entry key="checkstyle-version" value="10.12.3" />
         <entry key="copy-libs" value="false" />
         <entry key="location-0" value="BUNDLED:(bundled):Sun Checks" />
         <entry key="location-1" value="BUNDLED:(bundled):Google Checks" />
-        <entry key="location-2" value="HTTP_URL:https://gitlab.gistools.geog.uni-heidelberg.de/giscience/big-data/ohsome/parent/raw/2.13.0/ohsome-codestyle/src/main/resources/checkstyle-google-ohsome.xml:ohsome Google Checks" />
+        <entry key="location-2" value="HTTP_URL:https://gitlab.gistools.geog.uni-heidelberg.de/giscience/big-data/ohsome/parent/raw/2.14.0/ohsome-codestyle/src/main/resources/checkstyle-google-ohsome.xml:ohsome Google Checks" />
         <entry key="property-2.org.checkstyle.google.suppressionfilter.config" value="" />
         <entry key="property-2.org.checkstyle.google.suppressionxpathfilter.config" value="" />
         <entry key="scan-before-checkin" value="false" />
diff --git a/CHANGELOG.md b/CHANGELOG.md
index 0f2b07d6..e4f1d733 100644
--- a/CHANGELOG.md
+++ b/CHANGELOG.md
@@ -1,7 +1,16 @@
 Changelog
 =========
 
+## 1.10.0-SNAPSHOT (current master)
+
+### Bug Fixes
+* Fix bug which prevented some filters (which only work on ContributionView-based queries) to correctly work in contribution extraction endpoints ([#305])
+
+[#305]: https://github.com/GIScience/ohsome-api/issues/305
+
+
 ## 1.9.1
+
 * Upgrade OSHDB to version 1.1.2 [#302]
 
 [#302]: https://github.com/GIScience/ohsome-api/issues/302
diff --git a/Jenkinsfile b/Jenkinsfile
index 31e914f0..6f584c18 100644
--- a/Jenkinsfile
+++ b/Jenkinsfile
@@ -3,6 +3,9 @@ pipeline {
   options {
     timeout(time: 30, unit: 'MINUTES')
   }
+  tools {
+    maven 'Maven 3'
+  }
 
   environment {
     REPO_NAME = sh(returnStdout: true, script: 'basename `git remote get-url origin` .git').trim()
@@ -12,7 +15,7 @@ pipeline {
 
     MAVEN_GENERAL_OPTIONS = '--batch-mode --update-snapshots'
     // START CUSTOM ohsome API
-    MAVEN_TEST_OPTIONS = '-Dport_get=8081 -Dport_post=8082 -Dport_data=8083 -DdbFilePathProperty="--database.db=/opt/data/heidelberg-v1.0-beta.oshdb"'
+    MAVEN_TEST_OPTIONS = '-Dport_get=8081 -Dport_post=8082 -Dport_data=8083 -DdbFilePathProperty=--database.db=/opt/data/heidelberg-v1.0-beta.oshdb'
     // END CUSTOM ohsome API
     SNAPSHOT_BRANCH_REGEX = /(^master$)/
     RELEASE_REGEX = /^([0-9]+(\.[0-9]+)*)(-(RC|beta-|alpha-)[0-9]+)?$/
@@ -41,15 +44,8 @@ pipeline {
           }
         }
         script {
-          server = Artifactory.server 'HeiGIT Repo'
-          rtMaven = Artifactory.newMavenBuild()
-
-          rtMaven.resolver server: server, releaseRepo: 'main', snapshotRepo: 'main'
-          rtMaven.deployer server: server, releaseRepo: 'libs-release-local', snapshotRepo: 'libs-snapshot-local'
-          rtMaven.deployer.deployArtifacts = false
-
           withCredentials([string(credentialsId: 'gpg-signing-key-passphrase', variable: 'PASSPHRASE')]) {
-            buildInfo = rtMaven.run pom: 'pom.xml', goals: '$MAVEN_GENERAL_OPTIONS clean compile javadoc:jar source:jar verify -P jacoco,sign,git -Dmaven.repo.local=.m2 $MAVEN_TEST_OPTIONS -Dgpg.passphrase=$PASSPHRASE'
+            sh 'mvn $MAVEN_GENERAL_OPTIONS clean compile javadoc:jar source:jar verify -P jacoco,sign,git -Dmaven.repo.local=.m2 $MAVEN_TEST_OPTIONS -Dgpg.passphrase=$PASSPHRASE'
           }
         }
       }
@@ -90,7 +86,7 @@ pipeline {
           sh "mkdir -p ${report_dir} && rm -Rf ${report_dir}* && find . -path '*/target/site/jacoco' -exec cp -R --parents {} ${report_dir} \\; && find ${report_dir} -path '*/target/site/jacoco' | while read line; do echo \$line; neu=\${line/target\\/site\\/jacoco/} ;  mv \$line/* \$neu ; done && find ${report_dir} -type d -empty -delete"
 
           // warnings plugin
-          rtMaven.run pom: 'pom.xml', goals: '$MAVEN_GENERAL_OPTIONS -V -e compile checkstyle:checkstyle pmd:pmd pmd:cpd spotbugs:spotbugs -Dmaven.repo.local=.m2 $MAVEN_TEST_OPTIONS'
+          sh 'mvn $MAVEN_GENERAL_OPTIONS -V -e compile checkstyle:checkstyle pmd:pmd pmd:cpd spotbugs:spotbugs -Dmaven.repo.local=.m2 $MAVEN_TEST_OPTIONS'
 
           recordIssues enabledForFailure: true, tools: [mavenConsole(),  java(), javaDoc()]
           recordIssues enabledForFailure: true, tool: checkStyle()
@@ -114,11 +110,12 @@ pipeline {
       }
       steps {
         script {
-          withCredentials([string(credentialsId: 'gpg-signing-key-passphrase', variable: 'PASSPHRASE')]) {
-            buildInfo = rtMaven.run pom: 'pom.xml', goals: '$MAVEN_GENERAL_OPTIONS clean compile javadoc:jar source:jar install -P sign,git -Dmaven.repo.local=.m2 $MAVEN_TEST_OPTIONS -Dgpg.passphrase=$PASSPHRASE -DskipTests=true'
+          withCredentials([
+              file(credentialsId: 'nexus-settings', variable: 'settingsFile'),
+              string(credentialsId: 'gpg-signing-key-passphrase', variable: 'PASSPHRASE')
+          ]) {
+            sh 'mvn $MAVEN_GENERAL_OPTIONS clean compile -s $settingsFile javadoc:jar source:jar deploy -P sign,git -Dmaven.repo.local=.m2 $MAVEN_TEST_OPTIONS -Dgpg.passphrase=$PASSPHRASE -DskipTests=true'
           }
-          rtMaven.deployer.deployArtifacts buildInfo
-          server.publishBuildInfo buildInfo
           SNAPSHOT_DEPLOY = true
         }
       }
@@ -137,11 +134,12 @@ pipeline {
       }
       steps {
         script {
-          withCredentials([string(credentialsId: 'gpg-signing-key-passphrase', variable: 'PASSPHRASE')]) {
-            buildInfo = rtMaven.run pom: 'pom.xml', goals: '$MAVEN_GENERAL_OPTIONS clean compile javadoc:jar source:jar install -P sign,git -Dmaven.repo.local=.m2 $MAVEN_TEST_OPTIONS -Dgpg.passphrase=$PASSPHRASE -DskipTests=true'
+          withCredentials([
+              file(credentialsId: 'nexus-settings', variable: 'settingsFile'),
+              string(credentialsId: 'gpg-signing-key-passphrase', variable: 'PASSPHRASE')
+          ]) {
+            sh 'mvn $MAVEN_GENERAL_OPTIONS clean compile -s $settingsFile javadoc:jar source:jar deploy -P sign,git -Dmaven.repo.local=.m2 $MAVEN_TEST_OPTIONS -Dgpg.passphrase=$PASSPHRASE -DskipTests=true'
           }
-          rtMaven.deployer.deployArtifacts buildInfo
-          server.publishBuildInfo buildInfo
           RELEASE_DEPLOY = true
         }
         withCredentials([
@@ -168,12 +166,12 @@ pipeline {
       steps {
         script {
           // load dependencies to artifactory
-          rtMaven.run pom: 'pom.xml', goals: '$MAVEN_GENERAL_OPTIONS org.apache.maven.plugins:maven-help-plugin:2.1.1:evaluate -Dexpression=project.version -Dmaven.repo.local=.m2 $MAVEN_TEST_OPTIONS'
+          sh 'mvn $MAVEN_GENERAL_OPTIONS org.apache.maven.plugins:maven-help-plugin:2.1.1:evaluate -Dexpression=project.version -Dmaven.repo.local=.m2 $MAVEN_TEST_OPTIONS'
 
           javadc_dir = "/srv/javadoc/java/" + REPO_NAME + "/" + VERSION + "/"
           echo javadc_dir
 
-          rtMaven.run pom: 'pom.xml', goals: '$MAVEN_GENERAL_OPTIONS clean javadoc:javadoc -Dmaven.repo.local=.m2 $MAVEN_TEST_OPTIONS'
+          sh 'mvn $MAVEN_GENERAL_OPTIONS clean javadoc:javadoc -Dmaven.repo.local=.m2 $MAVEN_TEST_OPTIONS'
           sh "echo ${javadc_dir}"
           // make sure jenkins uses bash not dash!
           sh "mkdir -p ${javadc_dir} && rm -Rf ${javadc_dir}* && find . -path '*/target/site/apidocs' -exec cp -R --parents {} ${javadc_dir} \\; && find ${javadc_dir} -path '*/target/site/apidocs' | while read line; do echo \$line; neu=\${line/target\\/site\\/apidocs/} ;  mv \$line/* \$neu ; done && find ${javadc_dir} -type d -empty -delete"
diff --git a/pom.xml b/pom.xml
index e562a51f..5e01d146 100644
--- a/pom.xml
+++ b/pom.xml
@@ -6,7 +6,7 @@
   <parent>
     <groupId>org.heigit.ohsome</groupId>
     <artifactId>ohsome-parent</artifactId>
-    <version>2.13.1</version>
+    <version>2.14.0</version>
   </parent>
 
   <artifactId>ohsome-api</artifactId>
@@ -282,19 +282,11 @@
 
   <repositories>
     <repository>
-      <!--This will resolve artefacts of Osgeo, Boundless and potentially others through our own repository (https://www.jfrog.com/confluence/display/RTF/Maven+Repository#MavenRepository-ResolvingArtifactsthroughArtifactory).-->
-      <id>HeiGIT main</id>
-      <name>Central Repository for OSHDB dependency related artefacts</name>
-      <url>https://repo.heigit.org/artifactory/main</url>
+      <id>heigit-nexus-public</id>
+      <name>HeiGIT maven repositories</name>
+      <url>https://nexus.heigit.org/repository/maven-public/</url>
     </repository>
   </repositories>
-  <pluginRepositories>
-    <pluginRepository>
-      <id>oshdb-respository</id>
-      <name>Heigit/GIScience maven repository</name>
-      <url>https://repo.heigit.org/artifactory/main</url>
-    </pluginRepository>
-  </pluginRepositories>
 
   <issueManagement>
     <system>GitHub</system>
diff --git a/src/main/lombok/org/heigit/ohsome/ohsomeapi/executor/DataRequestExecutor.java b/src/main/lombok/org/heigit/ohsome/ohsomeapi/executor/DataRequestExecutor.java
index 992b8655..ff875058 100644
--- a/src/main/lombok/org/heigit/ohsome/ohsomeapi/executor/DataRequestExecutor.java
+++ b/src/main/lombok/org/heigit/ohsome/ohsomeapi/executor/DataRequestExecutor.java
@@ -24,6 +24,7 @@
 import org.heigit.ohsome.oshdb.api.db.OSHDBIgnite.ComputeMode;
 import org.heigit.ohsome.oshdb.api.mapreducer.MapReducer;
 import org.heigit.ohsome.oshdb.filter.FilterExpression;
+import org.heigit.ohsome.oshdb.util.mappable.OSHDBMapReducible;
 import org.heigit.ohsome.oshdb.util.mappable.OSMContribution;
 import org.heigit.ohsome.oshdb.util.mappable.OSMEntitySnapshot;
 import org.heigit.ohsome.oshdb.util.tagtranslator.TagTranslator;
@@ -61,20 +62,7 @@ public DataRequestExecutor(RequestResource requestResource, ElementsGeometry ele
    */
   public void extract() throws Exception {
     inputProcessor.getProcessingData().setFullHistory(true);
-    InputProcessor snapshotInputProcessor = new InputProcessor(servletRequest, true, false);
-    snapshotInputProcessor.getProcessingData().setFullHistory(true);
-    MapReducer<OSMEntitySnapshot> mapRedSnapshot = null;
-    MapReducer<OSMContribution> mapRedContribution = null;
-    if (DbConnData.db instanceof OSHDBIgnite) {
-      // on ignite: Use AffinityCall backend, which is the only one properly supporting streaming
-      // of result data, without buffering the whole result in memory before returning the result.
-      // This allows to write data out to the client via a chunked HTTP response.
-      mapRedSnapshot = snapshotInputProcessor.processParameters(ComputeMode.AFFINITY_CALL);
-      mapRedContribution = inputProcessor.processParameters(ComputeMode.AFFINITY_CALL);
-    } else {
-      mapRedSnapshot = snapshotInputProcessor.processParameters();
-      mapRedContribution = inputProcessor.processParameters();
-    }
+    final MapReducer<List<OSMContribution>> mapRedContributions = getMapReducer(inputProcessor);
     RequestParameters requestParameters = processingData.getRequestParameters();
     String[] time = inputProcessor.splitParamOnComma(
         inputProcessor.createEmptyArrayIfNull(servletRequest.getParameterValues("time")));
@@ -102,17 +90,10 @@ public void extract() throws Exception {
         .format(DateTimeFormatter.ISO_DATE_TIME);
     String endTimestamp = IsoDateTimeParser.parseIsoDateTime(requestParameters.getTime()[1])
         .format(DateTimeFormatter.ISO_DATE_TIME);
-    MapReducer<List<OSMContribution>> mapRedContributions = mapRedContribution.groupByEntity();
-    MapReducer<List<OSMEntitySnapshot>> mapRedSnapshots = mapRedSnapshot.groupByEntity();
-    Optional<FilterExpression> filter = processingData.getFilterExpression();
-    if (filter.isPresent()) {
-      mapRedSnapshots = mapRedSnapshots.filter(filter.get());
-      mapRedContributions = mapRedContributions.filter(filter.get());
-    }
     final boolean isContainingSimpleFeatureTypes = processingData.isContainingSimpleFeatureTypes();
     DataExtractionTransformer dataExtractionTransformer = new DataExtractionTransformer(
-        startTimestamp, endTimestamp, filter.orElse(null), isContributionsEndpoint,
-        isContributionsLatestEndpoint,
+        startTimestamp, endTimestamp, processingData.getFilterExpression().orElse(null),
+        isContributionsEndpoint, isContributionsLatestEndpoint,
         clipGeometries, includeTags, includeOSMMetadata, includeContributionTypes, utils, exeUtils,
         keysInt, elementsGeometry, simpleFeatureTypes,
         isContainingSimpleFeatureTypes);
@@ -128,6 +109,9 @@ public void extract() throws Exception {
         metadata, "FeatureCollection", Collections.emptyList());
     MapReducer<Feature> snapshotPreResult = null;
     if (!isContributionsEndpoint) {
+      InputProcessor snapshotInputProcessor = new InputProcessor(servletRequest, true, false);
+      snapshotInputProcessor.getProcessingData().setFullHistory(true);
+      MapReducer<List<OSMEntitySnapshot>> mapRedSnapshots = getMapReducer(snapshotInputProcessor);
       // handles cases where valid_from = t_start, valid_to = t_end; i.e. non-modified data
       snapshotPreResult = mapRedSnapshots
           .filter(snapshots -> snapshots.size() == 2)
@@ -146,4 +130,21 @@ public void extract() throws Exception {
           Stream.concat(contributionStream, snapshotStream));
     }
   }
+
+  private <X extends OSHDBMapReducible> MapReducer<List<X>> getMapReducer(
+      InputProcessor inputProcessor) throws Exception {
+    MapReducer<X> mapRed;
+    if (DbConnData.db instanceof OSHDBIgnite) {
+      // on ignite: Use AffinityCall backend, which is the only one properly supporting streaming
+      // of result data, without buffering the whole result in memory before returning the result.
+      // This allows to write data out to the client via a chunked HTTP response.
+      mapRed = inputProcessor.processParameters(ComputeMode.AFFINITY_CALL);
+    } else {
+      mapRed = inputProcessor.processParameters();
+    }
+    MapReducer<List<X>> mapRedGrouped = mapRed.groupByEntity();
+    Optional<FilterExpression> filter = processingData.getFilterExpression();
+    return filter.map(mapRedGrouped::filter)
+        .orElse(mapRedGrouped);
+  }
 }
diff --git a/src/test/java/org/heigit/ohsome/ohsomeapi/controller/DataExtractionTest.java b/src/test/java/org/heigit/ohsome/ohsomeapi/controller/DataExtractionTest.java
index 15a05369..feb31cdd 100644
--- a/src/test/java/org/heigit/ohsome/ohsomeapi/controller/DataExtractionTest.java
+++ b/src/test/java/org/heigit/ohsome/ohsomeapi/controller/DataExtractionTest.java
@@ -460,6 +460,20 @@ public void contributionTypesPropertiesParameterTest() {
     assertEquals("14227603", feature.get("properties").get("@contributionChangesetId").asText());
   }
 
+  @Test
+  public void contributionsChangesetFilterTest() {
+    TestRestTemplate restTemplate = new TestRestTemplate();
+    ResponseEntity<JsonNode> response = restTemplate.getForEntity(server + port
+        + "/contributions/bbox?bboxes=8.67,49.39,8.71,49.42&clipGeometry=true&"
+        + "filter=id:way/25316163 and changeset:14227603&"
+        + "properties=metadata,contributionTypes&time=2012-12-10,2017-12-11",
+        JsonNode.class);
+    var features = response.getBody().get("features");
+    assertEquals(1, features.size());
+    assertEquals("14227603",
+        features.get(0).get("properties").get("@contributionChangesetId").asText());
+  }
+
   /*
    * ./contributions/latest tests
    */
