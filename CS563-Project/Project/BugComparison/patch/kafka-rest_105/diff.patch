diff --git a/docs/api.rst b/docs/api.rst
index 5a533bd84c..d24c0e8240 100644
--- a/docs/api.rst
+++ b/docs/api.rst
@@ -89,6 +89,8 @@ per-request basis.
           * Error code 50002 -- Kafka error.
           * Error code 50003 -- Retriable Kafka error. Although the operation failed, it's
             possible that retrying the request will be successful.
+          * Error code 50101 -- Only SSL endpoints were found for the specified broker, but
+            SSL is not supported for the invoked API yet.
 
 Topics
 ------
@@ -201,7 +203,11 @@ you produce messages by making ``POST`` requests to specific topics.
 .. http:post:: /topics/(string:topic_name)
 
    Produce messages to a topic, optionally specifying keys or partitions for the
-   messages. For the ``avro`` embedded format, you must provide information
+   messages. If no partition is provided, one will be chosen based on the hash of
+   the key. If no key is provided, the partition will be chosen for each message
+   in a round-robin fashion.
+
+   For the ``avro`` embedded format, you must provide information
    about schemas and the REST proxy must be configured with the URL to access
    the schema registry (``schema.registry.connect``). Schemas may be provided as
    the full schema encoded as a string, or, after the initial request may be
diff --git a/docs/changelog.rst b/docs/changelog.rst
new file mode 100644
index 0000000000..b2848e1944
--- /dev/null
+++ b/docs/changelog.rst
@@ -0,0 +1,22 @@
+.. _kafkarest_changelog:
+
+Changelog
+=========
+
+Version 2.0.0
+-------------
+
+* `PR-64 <https://github.com/confluentinc/kafka-rest/pull/>`_ - Reduce integration test time.
+* `PR-66 <https://github.com/confluentinc/kafka-rest/pull/>`_ - Add support for SimpleConsumer-like access (Issue #26)
+* `PR-67 <https://github.com/confluentinc/kafka-rest/pull/>`_ - Handle conflicting IDs and separate IDs used in the REST proxy and by Kafka's consumer implementation.
+* `PR-78 <https://github.com/confluentinc/kafka-rest/pull/>`_ - Remove kafka from list of production directories to include in CLASSPATH.
+* `PR-89 <https://github.com/confluentinc/kafka-rest/pull/>`_ - JSON message support
+* `PR-96 <https://github.com/confluentinc/kafka-rest/pull/>`_ - Fixed log4j and daemon flag bugs in kafka-rest-run-class based on fix from schema-registry.
+* `PR-99 <https://github.com/confluentinc/kafka-rest/pull/>`_ - Require Java 7
+* `PR-101 <https://github.com/confluentinc/kafka-rest/pull/>`_ - rest-utils updates
+* `PR-103 <https://github.com/confluentinc/kafka-rest/pull/>`_ - Issue 94 rename main
+* `PR-108 <https://github.com/confluentinc/kafka-rest/pull/>`_ - Clarify partitioning behavior for produce requests
+* `PR-117 <https://github.com/confluentinc/kafka-rest/pull/>`_ - Update to Kafka 0.9.0.0-SNAPSHOT and make adjustments to work with updated ZkUtils.
+* `PR-122 <https://github.com/confluentinc/kafka-rest/pull/>`_ - Use x.y.z versioning scheme (i.e. 2.0.0-SNAPSHOT)
+* `PR-123 <https://github.com/confluentinc/kafka-rest/pull/>`_ - Updated args for JaasUtils.isZkSecurityEnabled()
+* `PR-125 <https://github.com/confluentinc/kafka-rest/pull/>`_ - Use Kafka compiled with Scala 2.11
diff --git a/docs/conf.py b/docs/conf.py
index bc5e532d70..512542c708 100644
--- a/docs/conf.py
+++ b/docs/conf.py
@@ -59,7 +59,7 @@ def setup(app):
 # The short X.Y version.
 version = '2.0'
 # The full version, including alpha/beta/rc tags.
-release = '2.0-SNAPSHOT'
+release = '2.0.0-SNAPSHOT'
 
 # The language for content autogenerated by Sphinx. Refer to documentation
 # for a list of supported languages.
diff --git a/docs/config.rst b/docs/config.rst
index f7c6dc9fdd..6e7e7f297a 100644
--- a/docs/config.rst
+++ b/docs/config.rst
@@ -69,6 +69,13 @@ Java Kafka clients.
   * Default: 25
   * Importance: medium
 
+``access.control.allow.origin``
+  Set value for Jetty Access-Control-Allow-Origin header
+
+  * Type: string
+  * Default: ""
+  * Importance: low
+
 ``consumer.instance.timeout.ms``
   Amount of idle time before a consumer instance is automatically destroyed.
 
@@ -108,7 +115,7 @@ Java Kafka clients.
   Prefix to apply to metric names for the default JMX reporter.
 
   * Type: string
-  * Default: "kafka-rest"
+  * Default: "kafka.rest"
   * Importance: low
 
 ``metrics.num.samples``
@@ -172,4 +179,4 @@ Java Kafka clients.
 
   * Type: int
   * Default: 1000
-  * Importance: low
\ No newline at end of file
+  * Importance: low
diff --git a/docs/index.rst b/docs/index.rst
index 4438769ac7..c0c742075a 100644
--- a/docs/index.rst
+++ b/docs/index.rst
@@ -12,6 +12,7 @@ Contents:
    :maxdepth: 3
 
    intro
+   changelog
    api
    config
    operations
diff --git a/docs/intro.rst b/docs/intro.rst
index 98b234e0a3..cf83a5fc99 100644
--- a/docs/intro.rst
+++ b/docs/intro.rst
@@ -13,8 +13,20 @@ framework that doesn't yet support Kafka, and scripting administrative actions.
 Quickstart
 ----------
 
-The following assumes you have Kafka, the schema registry, and an instance of
-the REST Proxy running using the default settings and some topics already created.
+Start by running the REST Proxy and the services it depends on: ZooKeeper, Kafka, and the Schema
+Registry:
+
+.. sourcecode:: bash
+
+   $ ./bin/zookeeper-server-start ./etc/kafka/zookeeper.properties &
+   $ ./bin/kafka-server-start ./etc/kafka/server.properties &
+   $ ./bin/schema-registry-start ./etc/schema-registry/schema-registry.properties &
+   $ ./bin/kafka-rest-start ./etc/kafka-rest/kafka-rest.properties &
+
+.. ifconfig:: platform_docs
+
+   See the :ref:`Confluent Platform quickstart<quickstart>` for a more detailed explanation of how
+   to get these services up and running.
 
 Inspect Topic Metadata
 ~~~~~~~~~~~~~~~~~~~~~~
@@ -23,15 +35,43 @@ Inspect Topic Metadata
 
    # Get a list of topics
    $ curl "http://localhost:8082/topics"
-     [{"name":"test","num_partitions":3},{"name":"test2","num_partitions":1}]
+     ["test","test2","test3"]
 
    # Get info about one topic
    $ curl "http://localhost:8082/topics/test"
-     {"name":"test","num_partitions":3}
+     {"test":"connect-test","configs":{},"partitions":[{"partition":0,"leader":0,"replicas":[{"broker":0,"leader":true,"in_sync":true}]},{"partition":1,"leader":0,"replicas":[{"broker":1,"leader":true,"in_sync":true}]}]}
 
    # Get info about a topic's partitions
    $ curl "http://localhost:8082/topics/test/partitions
-     [{"partition":0,"leader":1002,"replicas":[{"broker":1002,"leader":true,"in_sync":true}]}]
+     [{"partition":0,"leader":0,"replicas":[{"broker":0,"leader":true,"in_sync":true}]},{"partition":1,"leader":0,"replicas":[{"broker":1,"leader":true,"in_sync":true}]}]
+
+Produce and Consume Avro Messages
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+.. sourcecode:: bash
+
+   # Produce a message using Avro embedded data, including the schema which will
+   # be registered with the schema registry and used to validate and serialize
+   # before storing the data in Kafka
+   $ curl -X POST -H "Content-Type: application/vnd.kafka.avro.v1+json" \
+         --data '{"value_schema": "{\"type\": \"record\", \"name\": \"User\", \"fields\": [{\"name\": \"name\", \"type\": \"string\"}]}", "records": [{"value": {"name": "testUser"}}]}' \
+         "http://localhost:8082/topics/avrotest"
+     {"offsets":[{"partition":0,"offset":0,"error_code":null,"error":null}],"key_schema_id":null,"value_schema_id":21}
+
+   # Create a consumer for Avro data, starting at the beginning of the topic's
+   # log. Then consume some data from a topic, which is decoded, translated to
+   # JSON, and included in the response. The schema used for deserialization is
+   # fetched automatically from the schema registry. Finally, clean up.
+   $ curl -X POST -H "Content-Type: application/vnd.kafka.v1+json" \
+         --data '{"name": "my_consumer_instance", "format": "avro", "auto.offset.reset": "smallest"}' \
+         http://localhost:8082/consumers/my_avro_consumer
+     {"instance_id":"my_consumer_instance","base_uri":"http://localhost:8082/consumers/my_avro_consumer/instances/my_consumer_instance"}
+   $ curl -X GET -H "Accept: application/vnd.kafka.avro.v1+json" \
+         http://localhost:8082/consumers/my_avro_consumer/instances/my_consumer_instance/topics/avrotest
+     [{"key":null,"value":{"name":"testUser"},"partition":0,"offset":0}]
+   $ curl -X DELETE \
+         http://localhost:8082/consumers/my_avro_consumer/instances/my_consumer_instance
+     # No content in response
 
 Produce and Consume JSON Messages
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
@@ -40,7 +80,7 @@ Produce and Consume JSON Messages
 
    # Produce a message using JSON with the value '{ "foo": "bar" }' to the topic test
    $ curl -X POST -H "Content-Type: application/vnd.kafka.json.v1+json" \
-         --data '{"records":[{"value":{"foo":"bar"}]}' "http://localhost:8082/topics/test"
+         --data '{"records":[{"value":{"foo":"bar"}}]}' "http://localhost:8082/topics/jsontest"
      {"offsets":[{"partition":0,"offset":0,"error_code":null,"error":null}],"key_schema_id":null,"value_schema_id":null}
 
    # Create a consumer for JSON data, starting at the beginning of the topic's
@@ -48,14 +88,15 @@ Produce and Consume JSON Messages
    # Finally, close the consumer with a DELETE to make it leave the group and clean up
    # its resources.
    $ curl -X POST -H "Content-Type: application/vnd.kafka.v1+json" \
-         --data '{"format": "json", "auto.offset.reset": "smallest"}' \
+         --data '{"name": "my_consumer_instance", "format": "json", "auto.offset.reset": "smallest"}' \
          http://localhost:8082/consumers/my_json_consumer
-     {"instance_id":"rest-consumer-11561681-8ba5-4b46-bed0-905ae1769bc6","base_uri":"http://localhost:8082/consumers/my_json_consumer/instances/rest-consumer-11561681-8ba5-4b46-bed0-905ae1769bc6"}
+     {"instance_id":"my_consumer_instance",
+     "base_uri":"http://localhost:8082/consumers/my_json_consumer/instances/my_consumer_instance"}
    $ curl -X GET -H "Accept: application/vnd.kafka.json.v1+json" \
-         http://localhost:8082/consumers/my_json_consumer/instances/rest-consumer-11561681-8ba5-4b46-bed0-905ae1769bc6/topics/test
+         http://localhost:8082/consumers/my_json_consumer/instances/my_consumer_instance/topics/jsontest
      [{"key":null,"value":{"foo":"bar"},"partition":0,"offset":0}]
    $ curl -X DELETE \
-         http://localhost:8082/consumers/my_json_consumer/instances/rest-consumer-11561681-8ba5-4b46-bed0-905ae1769bc6
+         http://localhost:8082/consumers/my_json_consumer/instances/my_consumer_instance
      # No content in response
 
 Produce and Consume Binary Messages
@@ -65,7 +106,7 @@ Produce and Consume Binary Messages
 
    # Produce a message using binary embedded data with value "Kafka" to the topic test
    $ curl -X POST -H "Content-Type: application/vnd.kafka.binary.v1+json" \
-         --data '{"records":[{"value":"S2Fma2E="}]}' "http://localhost:8082/topics/test"
+         --data '{"records":[{"value":"S2Fma2E="}]}' "http://localhost:8082/topics/binarytest"
      {"offsets":[{"partition":0,"offset":0,"error_code":null,"error":null}],"key_schema_id":null,"value_schema_id":null}
 
    # Create a consumer for binary data, starting at the beginning of the topic's
@@ -73,42 +114,14 @@ Produce and Consume Binary Messages
    # Finally, close the consumer with a DELETE to make it leave the group and clean up
    # its resources.
    $ curl -X POST -H "Content-Type: application/vnd.kafka.v1+json" \
-         --data '{"format": "binary", "auto.offset.reset": "smallest"}' \
+         --data '{"name": "my_consumer_instance", "format": "binary", "auto.offset.reset": "smallest"}' \
          http://localhost:8082/consumers/my_binary_consumer
-     {"instance_id":"rest-consumer-11561681-8ba5-4b46-bed0-905ae1769bc6","base_uri":"http://localhost:8082/consumers/my_binary_consumer/instances/rest-consumer-11561681-8ba5-4b46-bed0-905ae1769bc6"}
+     {"instance_id":"my_consumer_instance","base_uri":"http://localhost:8082/consumers/my_binary_consumer/instances/my_consumer_instance"}
    $ curl -X GET -H "Accept: application/vnd.kafka.binary.v1+json" \
-         http://localhost:8082/consumers/my_binary_consumer/instances/rest-consumer-11561681-8ba5-4b46-bed0-905ae1769bc6/topics/test
+         http://localhost:8082/consumers/my_binary_consumer/instances/my_consumer_instance/topics/binarytest
      [{"key":null,"value":"S2Fma2E=","partition":0,"offset":0}]
    $ curl -X DELETE \
-         http://localhost:8082/consumers/my_binary_consumer/instances/rest-consumer-11561681-8ba5-4b46-bed0-905ae1769bc6
-     # No content in response
-
-Produce and Consume Avro Messages
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-.. sourcecode:: bash
-
-   # Produce a message using Avro embedded data, including the schema which will
-   # be registered with the schema registry and used to validate and serialize
-   # before storing the data in Kafka
-   $ curl -X POST -H "Content-Type: application/vnd.kafka.avro.v1+json" \
-         --data '{"value_schema": "{\"type\": \"record\", \"name\": \"User\", \"fields\": [{\"name\": \"name\", \"type\": \"string\"}]}", "records": [{"value": {"name": "testUser"}}]}' \
-         "http://localhost:8082/topics/avrotest"
-     {"offsets":[{"partition":0,"offset":0,"error_code":null,"error":null}],"key_schema_id":null,"value_schema_id":21}
-
-   # Create a consumer for Avro data, starting at the beginning of the topic's
-   # log. Then consume some data from a topic, which is decoded, translated to
-   # JSON, and included in the response. The schema used for deserialization is
-   # fetched automatically from the schema registry. Finally, clean up.
-   $ curl -X POST -H "Content-Type: application/vnd.kafka.v1+json" \
-         --data '{"format": "avro", "auto.offset.reset": "smallest"}' \
-         http://localhost:8082/consumers/my_avro_consumer
-     {"instance_id":"rest-consumer-11392f3a-efbe-4fe2-b0bf-5c85d7b25e7b","base_uri":"http://localhost:8082/consumers/my_avro_consumer/instances/rest-consumer-11392f3a-efbe-4fe2-b0bf-5c85d7b25e7b"}
-   $ curl -X GET -H "Accept: application/vnd.kafka.avro.v1+json" \
-         http://localhost:8082/consumers/my_avro_consumer/instances/rest-consumer-11392f3a-efbe-4fe2-b0bf-5c85d7b25e7b/topics/avrotest
-     [{"key":null,"value":{"name":"testUser"},"partition":0,"offset":0}]
-   $ curl -X DELETE \
-         http://localhost:8082/consumers/my_avro_consumer/instances/rest-consumer-11392f3a-efbe-4fe2-b0bf-5c85d7b25e7b
+         http://localhost:8082/consumers/my_binary_consumer/instances/my_consumer_instance
      # No content in response
 
 Features
@@ -289,6 +302,12 @@ generating
 ``target/kafka-rest-$VERSION-standalone.jar``, which includes all the
 dependencies as well.
 
+Requirements
+------------
+
+- Kafka 0.9.0.0-cp1
+- Required for Avro support: Schema Registry 2.0.0 recommended, 1.0 minimum
+
 Contribute
 ----------
 
diff --git a/docs/requirements.txt b/docs/requirements.txt
index 6ff9a8162b..9e24690455 100644
--- a/docs/requirements.txt
+++ b/docs/requirements.txt
@@ -1,3 +1,3 @@
 Sphinx
 sphinx_rtd_theme
-sphinxcontrib-httpdomain
+sphinxcontrib-httpdomain>=1.4.0
diff --git a/pom.xml b/pom.xml
index 48260a2607..b093bb6a17 100644
--- a/pom.xml
+++ b/pom.xml
@@ -7,7 +7,7 @@
     <groupId>io.confluent</groupId>
     <artifactId>kafka-rest</artifactId>
     <packaging>jar</packaging>
-    <version>2.0-SNAPSHOT</version>
+    <version>2.0.0-SNAPSHOT</version>
     <name>kafka-rest</name>
     <organization>
         <name>Confluent, Inc.</name>
@@ -36,20 +36,21 @@
     </scm>
 
     <properties>
-        <confluent.version>2.0-SNAPSHOT</confluent.version>
-        <kafka.version>0.8.2.0-cp</kafka.version>
-        <kafka.scala.version>2.10</kafka.scala.version>
+        <confluent.version>2.0.0-SNAPSHOT</confluent.version>
+        <kafka.version>0.9.0.0</kafka.version>
+        <kafka.scala.version>2.11</kafka.scala.version>
         <!-- See note about workaround below. This should match the version used in the version of rest-utils you're using. -->
         <confluent.junit.version>4.12</confluent.junit.version>
         <easymock.version>3.0</easymock.version>
         <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
+        <confluent.maven.repo>http://packages.confluent.io/maven/</confluent.maven.repo>
     </properties>
 
     <repositories>
         <repository>
             <id>confluent</id>
             <name>Confluent</name>
-            <url>http://packages.confluent.io/maven/</url>
+            <url>${confluent.maven.repo}</url>
         </repository>
     </repositories>
 
@@ -177,6 +178,7 @@
             <plugin>
                 <groupId>org.apache.maven.plugins</groupId>
                 <artifactId>maven-jar-plugin</artifactId>
+                <version>2.6</version>
                 <executions>
                     <execution>
                         <goals>
diff --git a/src/main/java/io/confluent/kafkarest/Errors.java b/src/main/java/io/confluent/kafkarest/Errors.java
index 6b8e874ecc..29d186ed04 100644
--- a/src/main/java/io/confluent/kafkarest/Errors.java
+++ b/src/main/java/io/confluent/kafkarest/Errors.java
@@ -164,6 +164,15 @@ public static RestServerErrorException kafkaRetriableErrorException(Throwable e)
                                         KAFKA_RETRIABLE_ERROR_ERROR_CODE);
   }
 
+  public final static String NO_SSL_SUPPORT_MESSAGE =
+      "Only SSL endpoints were found for the broker, but SSL is not currently supported.";
+  public final static int NO_SSL_SUPPORT_ERROR_CODE = 50101;
+
+  public static RestServerErrorException noSslSupportException() {
+    return new RestServerErrorException(NO_SSL_SUPPORT_MESSAGE,
+                                        NO_SSL_SUPPORT_ERROR_CODE);
+  }
+
   public final static String NO_SIMPLE_CONSUMER_AVAILABLE_ERROR_MESSAGE =
       "No SimpleConsumer is available at the time in the pool. The request can be retried. " +
       "You can increase the pool size or the pool timeout to avoid this error in the future.";
diff --git a/src/main/java/io/confluent/kafkarest/KafkaRestApplication.java b/src/main/java/io/confluent/kafkarest/KafkaRestApplication.java
index ae6b460d0b..8949226773 100644
--- a/src/main/java/io/confluent/kafkarest/KafkaRestApplication.java
+++ b/src/main/java/io/confluent/kafkarest/KafkaRestApplication.java
@@ -15,7 +15,7 @@
  **/
 package io.confluent.kafkarest;
 
-import org.I0Itec.zkclient.ZkClient;
+import org.apache.kafka.common.security.JaasUtils;
 
 import java.util.Properties;
 
@@ -29,14 +29,14 @@
 import io.confluent.kafkarest.resources.TopicsResource;
 import io.confluent.rest.Application;
 import io.confluent.rest.RestConfigException;
-import kafka.utils.ZKStringSerializer$;
+import kafka.utils.ZkUtils;
 
 /**
  * Utilities for configuring and running an embedded Kafka server.
  */
 public class KafkaRestApplication extends Application<KafkaRestConfig> {
 
-  ZkClient zkClient;
+  ZkUtils zkUtils;
   Context context;
 
   public KafkaRestApplication() throws RestConfigException {
@@ -61,22 +61,23 @@ public void setupResources(Configurable<?> config, KafkaRestConfig appConfig) {
    * can be customized for testing. This only exists to support TestKafkaRestApplication
    */
   protected void setupInjectedResources(Configurable<?> config, KafkaRestConfig appConfig,
-                                        ZkClient zkClient, MetadataObserver mdObserver,
+                                        ZkUtils zkUtils, MetadataObserver mdObserver,
                                         ProducerPool producerPool,
                                         ConsumerManager consumerManager,
                                         SimpleConsumerFactory simpleConsumerFactory,
                                         SimpleConsumerManager simpleConsumerManager) {
     config.register(new ZkExceptionMapper(appConfig));
 
-    if (zkClient == null) {
-      zkClient = new ZkClient(appConfig.getString(KafkaRestConfig.ZOOKEEPER_CONNECT_CONFIG),
-                              30000, 30000, ZKStringSerializer$.MODULE$);
+    if (zkUtils == null) {
+      zkUtils = ZkUtils.apply(
+          appConfig.getString(KafkaRestConfig.ZOOKEEPER_CONNECT_CONFIG), 30000, 30000,
+          JaasUtils.isZkSecurityEnabled());
     }
     if (mdObserver == null) {
-      mdObserver = new MetadataObserver(appConfig, zkClient);
+      mdObserver = new MetadataObserver(appConfig, zkUtils);
     }
     if (producerPool == null) {
-      producerPool = new ProducerPool(appConfig, zkClient);
+      producerPool = new ProducerPool(appConfig, zkUtils);
     }
     if (consumerManager == null) {
       consumerManager = new ConsumerManager(appConfig, mdObserver);
@@ -88,7 +89,7 @@ protected void setupInjectedResources(Configurable<?> config, KafkaRestConfig ap
       simpleConsumerManager = new SimpleConsumerManager(appConfig, mdObserver, simpleConsumerFactory);
     }
 
-    this.zkClient = zkClient;
+    this.zkUtils = zkUtils;
     context = new Context(appConfig, mdObserver, producerPool, consumerManager, simpleConsumerManager);
     config.register(RootResource.class);
     config.register(new BrokersResource(context));
@@ -103,6 +104,6 @@ public void onShutdown() {
     context.getProducerPool().shutdown();
     context.getSimpleConsumerManager().shutdown();
     context.getMetadataObserver().shutdown();
-    zkClient.close();
+    zkUtils.close();
   }
 }
diff --git a/src/main/java/io/confluent/kafkarest/MetadataObserver.java b/src/main/java/io/confluent/kafkarest/MetadataObserver.java
index a56fd832e3..1c5b2a9b59 100644
--- a/src/main/java/io/confluent/kafkarest/MetadataObserver.java
+++ b/src/main/java/io/confluent/kafkarest/MetadataObserver.java
@@ -15,7 +15,6 @@
  **/
 package io.confluent.kafkarest;
 
-import org.I0Itec.zkclient.ZkClient;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -48,14 +47,14 @@ public class MetadataObserver {
 
   private static final Logger log = LoggerFactory.getLogger(ConsumerWorker.class);
 
-  private ZkClient zkClient;
+  private ZkUtils zkUtils;
 
-  public MetadataObserver(KafkaRestConfig config, ZkClient zkClient) {
-    this.zkClient = zkClient;
+  public MetadataObserver(KafkaRestConfig config, ZkUtils zkUtils) {
+    this.zkUtils = zkUtils;
   }
 
   public List<Integer> getBrokerIds() {
-    Seq<Broker> brokers = ZkUtils.getAllBrokersInCluster(zkClient);
+    Seq<Broker> brokers = zkUtils.getAllBrokersInCluster();
     List<Integer> brokerIds = new Vector<Integer>(brokers.size());
     for (Broker broker : JavaConversions.asJavaCollection(brokers)) {
       brokerIds.add(broker.id());
@@ -64,7 +63,7 @@ public List<Integer> getBrokerIds() {
   }
 
   private Broker getBrokerById(final int brokerId) {
-    Option<Broker> broker = ZkUtils.getBrokerInfo(zkClient, brokerId);
+    Option<Broker> broker = zkUtils.getBrokerInfo(brokerId);
 
     if (broker.isDefined()) {
       return broker.get();
@@ -78,13 +77,13 @@ public Broker getLeader(final String topicName, final int partitionId) {
   }
 
   public Collection<String> getTopicNames() {
-    Seq<String> topicNames = ZkUtils.getAllTopics(zkClient).sorted(Ordering.String$.MODULE$);
+    Seq<String> topicNames = zkUtils.getAllTopics().sorted(Ordering.String$.MODULE$);
     return JavaConversions.asJavaCollection(topicNames);
   }
 
   public List<Topic> getTopics() {
     try {
-      Seq<String> topicNames = ZkUtils.getAllTopics(zkClient).sorted(Ordering.String$.MODULE$);
+      Seq<String> topicNames = zkUtils.getAllTopics().sorted(Ordering.String$.MODULE$);
       return getTopicsData(topicNames);
     } catch (RestNotFoundException e) {
       throw new InternalServerErrorException(e);
@@ -103,28 +102,30 @@ public boolean topicExists(String topicName) {
 
   public Topic getTopic(String topicName) {
     List<Topic> topics =
-        getTopicsData(JavaConversions.asScalaIterable(Arrays.asList(topicName)).toSeq());
+        getTopicsData(JavaConversions.asScalaBuffer(Arrays.asList(topicName)));
     return (topics.isEmpty() ? null : topics.get(0));
   }
 
   private List<Topic> getTopicsData(Seq<String> topicNames) {
     Map<String, Map<Object, Seq<Object>>> topicPartitions =
-        ZkUtils.getPartitionAssignmentForTopics(zkClient, topicNames);
+        zkUtils.getPartitionAssignmentForTopics(topicNames);
     List<Topic> topics = new Vector<Topic>(topicNames.size());
     // Admin utils only supports getting either 1 or all topic configs. These per-topic overrides
     // shouldn't be common, so we just grab all of them to keep this simple
-    Map<String, Properties> configs = AdminUtils.fetchAllTopicConfigs(zkClient);
+    Map<String, Properties> configs = AdminUtils.fetchAllTopicConfigs(zkUtils);
     for (String topicName : JavaConversions.asJavaCollection(topicNames)) {
-      Map<Object, Seq<Object>> partitionMap = topicPartitions.get(topicName).get();
-      List<Partition> partitions = extractPartitionsFromZKData(partitionMap, topicName, null);
-      if (partitions.size() == 0) {
-        continue;
+      if(!topicPartitions.get(topicName).isEmpty()) {
+        Map<Object, Seq<Object>> partitionMap = topicPartitions.get(topicName).get();
+        List<Partition> partitions = extractPartitionsFromZKData(partitionMap, topicName, null);
+        if (partitions.size() == 0) {
+          continue;
+        }
+        Option<Properties> topicConfigOpt = configs.get(topicName);
+        Properties topicConfigs =
+                topicConfigOpt.isEmpty() ? new Properties() : topicConfigOpt.get();
+        Topic topic = new Topic(topicName, topicConfigs, partitions);
+        topics.add(topic);
       }
-      Option<Properties> topicConfigOpt = configs.get(topicName);
-      Properties topicConfigs =
-          topicConfigOpt.isEmpty() ? new Properties() : topicConfigOpt.get();
-      Topic topic = new Topic(topicName, topicConfigs, partitions);
-      topics.add(topic);
     }
     return topics;
   }
@@ -147,10 +148,13 @@ public Partition getTopicPartition(String topic, int partition) {
   }
 
   private List<Partition> getTopicPartitions(String topic, Integer partitions_filter) {
-    Map<String, Map<Object, Seq<Object>>> topicPartitions = ZkUtils.getPartitionAssignmentForTopics(
-        zkClient, JavaConversions.asScalaIterable(Arrays.asList(topic)).toSeq());
-    Map<Object, Seq<Object>> parts = topicPartitions.get(topic).get();
-    return extractPartitionsFromZKData(parts, topic, partitions_filter);
+    Map<String, Map<Object, Seq<Object>>> topicPartitions = zkUtils.getPartitionAssignmentForTopics(
+            JavaConversions.asScalaBuffer(Arrays.asList(topic)));
+    if (!topicPartitions.get(topic).isEmpty()) {
+      Map<Object, Seq<Object>> parts = topicPartitions.get(topic).get();
+      return extractPartitionsFromZKData(parts, topic, partitions_filter);
+    }
+    return null;
   }
 
   public int getLeaderId(final String topicName, final int partitionId) {
@@ -172,7 +176,7 @@ public int getLeaderId(final String topicName, final int partitionId) {
   private List<Partition> extractPartitionsFromZKData(
       Map<Object, Seq<Object>> parts, String topic, Integer partitions_filter) {
     List<Partition> partitions = new Vector<Partition>();
-    java.util.Map<Object, Seq<Object>> partsJava = JavaConversions.asJavaMap(parts);
+    java.util.Map<Object, Seq<Object>> partsJava = JavaConversions.mapAsJavaMap(parts);
     for (java.util.Map.Entry<Object, Seq<Object>> part : partsJava.entrySet()) {
       int partId = (Integer) part.getKey();
       if (partitions_filter != null && partitions_filter != partId) {
@@ -181,20 +185,21 @@ private List<Partition> extractPartitionsFromZKData(
 
       Partition p = new Partition();
       p.setPartition(partId);
-      LeaderAndIsr leaderAndIsr =
-          ZkUtils.getLeaderAndIsrForPartition(zkClient, topic, partId).get();
-      p.setLeader(leaderAndIsr.leader());
-      scala.collection.immutable.Set<Integer> isr = leaderAndIsr.isr().toSet();
-      List<PartitionReplica> partReplicas = new Vector<PartitionReplica>();
-      for (Object brokerObj : JavaConversions.asJavaCollection(part.getValue())) {
-        int broker = (Integer) brokerObj;
-        PartitionReplica
-            r =
-            new PartitionReplica(broker, (leaderAndIsr.leader() == broker), isr.contains(broker));
-        partReplicas.add(r);
+      if(!zkUtils.getLeaderAndIsrForPartition(topic, partId).isEmpty()) {
+        LeaderAndIsr leaderAndIsr =
+                zkUtils.getLeaderAndIsrForPartition(topic, partId).get();
+        p.setLeader(leaderAndIsr.leader());
+        scala.collection.immutable.Set<Integer> isr = leaderAndIsr.isr().toSet();
+        List<PartitionReplica> partReplicas = new Vector<PartitionReplica>();
+        for (Object brokerObj : JavaConversions.asJavaCollection(part.getValue())) {
+          int broker = (Integer) brokerObj;
+          PartitionReplica r =
+                  new PartitionReplica(broker, (leaderAndIsr.leader() == broker), isr.contains(broker));
+          partReplicas.add(r);
+        }
+        p.setReplicas(partReplicas);
+        partitions.add(p);
       }
-      p.setReplicas(partReplicas);
-      partitions.add(p);
     }
     return partitions;
   }
diff --git a/src/main/java/io/confluent/kafkarest/ProducerPool.java b/src/main/java/io/confluent/kafkarest/ProducerPool.java
index 25f6b8d914..eea80f78a6 100644
--- a/src/main/java/io/confluent/kafkarest/ProducerPool.java
+++ b/src/main/java/io/confluent/kafkarest/ProducerPool.java
@@ -21,8 +21,8 @@
 import io.confluent.kafkarest.entities.ProduceRecord;
 import io.confluent.kafkarest.entities.SchemaHolder;
 import kafka.cluster.Broker;
+import kafka.cluster.EndPoint;
 import kafka.utils.ZkUtils;
-import org.I0Itec.zkclient.ZkClient;
 import org.apache.kafka.clients.producer.KafkaProducer;
 import org.apache.kafka.clients.producer.ProducerConfig;
 import org.apache.kafka.common.serialization.ByteArraySerializer;
@@ -49,13 +49,13 @@ public class ProducerPool {
   private Map<EmbeddedFormat, RestProducer> producers =
       new HashMap<EmbeddedFormat, RestProducer>();
 
-  public ProducerPool(KafkaRestConfig appConfig, ZkClient zkClient) {
-    this(appConfig, zkClient, null);
+  public ProducerPool(KafkaRestConfig appConfig, ZkUtils zkUtils) {
+    this(appConfig, zkUtils, null);
   }
 
-  public ProducerPool(KafkaRestConfig appConfig, ZkClient zkClient,
+  public ProducerPool(KafkaRestConfig appConfig, ZkUtils zkUtils,
                       Properties producerConfigOverrides) {
-    this(appConfig, getBootstrapBrokers(zkClient), producerConfigOverrides);
+    this(appConfig, getBootstrapBrokers(zkUtils), producerConfigOverrides);
   }
 
   public ProducerPool(KafkaRestConfig appConfig, String bootstrapBrokers,
@@ -133,14 +133,17 @@ private Map<String, Object> buildConfig(Map<String, Object> defaults,
     return config;
   }
 
-  private static String getBootstrapBrokers(ZkClient zkClient) {
-    Seq<Broker> brokerSeq = ZkUtils.getAllBrokersInCluster(zkClient);
+  private static String getBootstrapBrokers(ZkUtils zkUtils) {
+    Seq<Broker> brokerSeq = zkUtils.getAllBrokersInCluster();
+
     List<Broker> brokers = JavaConversions.seqAsJavaList(brokerSeq);
     String bootstrapBrokers = "";
     for (int i = 0; i < brokers.size(); i++) {
-      bootstrapBrokers += brokers.get(i).connectionString();
-      if (i != (brokers.size() - 1)) {
-        bootstrapBrokers += ",";
+      for(EndPoint ep : JavaConversions.asJavaCollection(brokers.get(i).endPoints().values())) {
+        if (bootstrapBrokers.length() > 0) {
+          bootstrapBrokers += ",";
+        }
+        bootstrapBrokers += ep.connectionString();
       }
     }
     return bootstrapBrokers;
diff --git a/src/main/java/io/confluent/kafkarest/SimpleConsumerManager.java b/src/main/java/io/confluent/kafkarest/SimpleConsumerManager.java
index 8749487428..0c3a375e05 100644
--- a/src/main/java/io/confluent/kafkarest/SimpleConsumerManager.java
+++ b/src/main/java/io/confluent/kafkarest/SimpleConsumerManager.java
@@ -27,6 +27,8 @@
 import io.confluent.rest.exceptions.RestServerErrorException;
 import kafka.api.PartitionFetchInfo;
 import kafka.cluster.Broker;
+import kafka.cluster.BrokerEndPoint;
+import kafka.common.BrokerEndPointNotAvailableException;
 import kafka.common.TopicAndPartition;
 import kafka.javaapi.FetchRequest;
 import kafka.javaapi.FetchResponse;
@@ -36,6 +38,8 @@
 import kafka.serializer.Decoder;
 import kafka.serializer.DefaultDecoder;
 import kafka.utils.VerifiableProperties;
+
+import org.apache.kafka.common.protocol.SecurityProtocol;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -104,7 +108,13 @@ private SimpleFetcher getSimpleFetcher(final Broker broker) {
       pool = simpleConsumersPools.get(broker);
     }
 
-    return pool.get(broker.host(), broker.port());
+    // TODO: Add support for SSL when simple consumer is changed to use new consumer
+    try {
+      BrokerEndPoint ep = broker.getBrokerEndPoint(SecurityProtocol.PLAINTEXT);
+      return pool.get(ep.host(), ep.port());
+    } catch (BrokerEndPointNotAvailableException e) {
+      throw Errors.noSslSupportException();
+    }
   }
 
   public void consume(final String topicName,
diff --git a/src/test/java/io/confluent/kafkarest/integration/AbstractConsumerTest.java b/src/test/java/io/confluent/kafkarest/integration/AbstractConsumerTest.java
index def66f4e7d..bb7af4e045 100644
--- a/src/test/java/io/confluent/kafkarest/integration/AbstractConsumerTest.java
+++ b/src/test/java/io/confluent/kafkarest/integration/AbstractConsumerTest.java
@@ -62,7 +62,7 @@ protected void produceBinaryMessages(List<ProducerRecord<byte[], byte[]>> record
     Properties props = new Properties();
     props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);
     props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);
-    props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
+    props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList);
     props.setProperty(ProducerConfig.ACKS_CONFIG, "all");
     Producer<byte[], byte[]> producer = new KafkaProducer<byte[], byte[]>(props);
     for (ProducerRecord<byte[], byte[]> rec : records) {
@@ -79,7 +79,7 @@ protected void produceJsonMessages(List<ProducerRecord<Object, Object>> records)
     Properties props = new Properties();
     props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, KafkaJsonSerializer.class);
     props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaJsonSerializer.class);
-    props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
+    props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList);
     props.setProperty(ProducerConfig.ACKS_CONFIG, "all");
     Producer<Object, Object> producer = new KafkaProducer<Object, Object>(props);
     for (ProducerRecord<Object, Object> rec : records) {
@@ -101,7 +101,7 @@ protected void produceAvroMessages(List<ProducerRecord<Object, Object>> records)
     avroValueSerializer.configure(serProps, false);
 
     Properties props = new Properties();
-    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
+    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList);
     props.put(ProducerConfig.ACKS_CONFIG, "all");
     props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);
     props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);
diff --git a/src/test/java/io/confluent/kafkarest/integration/AbstractProducerTest.java b/src/test/java/io/confluent/kafkarest/integration/AbstractProducerTest.java
index 591f5bdc3a..6aa3e54f82 100644
--- a/src/test/java/io/confluent/kafkarest/integration/AbstractProducerTest.java
+++ b/src/test/java/io/confluent/kafkarest/integration/AbstractProducerTest.java
@@ -77,7 +77,7 @@ protected <K, V> void testProduceToPartition(String topicName,
   }
 
 
-  protected <K, V> void testProduceToTopicFails(String topicName,
+  protected void testProduceToTopicFails(String topicName,
                                                 List<? extends TopicProduceRecord> records) {
     TopicProduceRequest payload = new TopicProduceRequest();
     payload.setRecords(records);
diff --git a/src/test/java/io/confluent/kafkarest/integration/AvroProducerTest.java b/src/test/java/io/confluent/kafkarest/integration/AvroProducerTest.java
index 38bdd98786..a508570df1 100644
--- a/src/test/java/io/confluent/kafkarest/integration/AvroProducerTest.java
+++ b/src/test/java/io/confluent/kafkarest/integration/AvroProducerTest.java
@@ -126,8 +126,8 @@ public void setUp() throws Exception {
     super.setUp();
     final int numPartitions = 3;
     final int replicationFactor = 1;
-    kafka.utils.TestUtils.createTopic(zkClient, topicName, numPartitions, replicationFactor,
-                                      JavaConversions.asScalaIterable(this.servers).toSeq(),
+    kafka.utils.TestUtils.createTopic(zkUtils, topicName, numPartitions, replicationFactor,
+                                      JavaConversions.asScalaBuffer(this.servers),
                                       new Properties());
 
     Properties props = new Properties();
diff --git a/src/test/java/io/confluent/kafkarest/integration/ClusterTestHarness.java b/src/test/java/io/confluent/kafkarest/integration/ClusterTestHarness.java
index 358708361f..acb9a7fca9 100644
--- a/src/test/java/io/confluent/kafkarest/integration/ClusterTestHarness.java
+++ b/src/test/java/io/confluent/kafkarest/integration/ClusterTestHarness.java
@@ -16,11 +16,16 @@
 package io.confluent.kafkarest.integration;
 
 import io.confluent.kafkarest.*;
-import org.I0Itec.zkclient.ZkClient;
+
+import org.apache.kafka.common.protocol.SecurityProtocol;
+import org.apache.kafka.common.security.JaasUtils;
 import org.eclipse.jetty.server.Server;
 import org.junit.After;
 import org.junit.Before;
 
+import java.io.File;
+import java.io.IOException;
+import java.net.ServerSocket;
 import java.net.URI;
 import java.net.URISyntaxException;
 import java.util.*;
@@ -35,11 +40,12 @@
 import io.confluent.kafka.schemaregistry.rest.SchemaRegistryRestApplication;
 import kafka.server.KafkaConfig;
 import kafka.server.KafkaServer;
+import kafka.utils.CoreUtils;
 import kafka.utils.SystemTime$;
 import kafka.utils.TestUtils;
-import kafka.utils.Utils;
-import kafka.utils.ZKStringSerializer$;
+import kafka.utils.ZkUtils;
 import kafka.zk.EmbeddedZookeeper;
+import scala.Option;
 import scala.collection.JavaConversions;
 
 /**
@@ -51,19 +57,45 @@ public abstract class ClusterTestHarness {
 
   public static final int DEFAULT_NUM_BROKERS = 1;
 
-  // Shared config
-  protected Queue<Integer> ports;
+  /**
+   * Choose a number of random available ports
+   */
+  public static int[] choosePorts(int count) {
+    try {
+      ServerSocket[] sockets = new ServerSocket[count];
+      int[] ports = new int[count];
+      for (int i = 0; i < count; i++) {
+        sockets[i] = new ServerSocket(0);
+        ports[i] = sockets[i].getLocalPort();
+      }
+      for (int i = 0; i < count; i++)
+        sockets[i].close();
+      return ports;
+    } catch (IOException e) {
+      throw new RuntimeException(e);
+    }
+  }
+
+  /**
+   * Choose an available port
+   */
+  public static int choosePort() {
+    return choosePorts(1)[0];
+  }
+
+
+  private int numBrokers;
+  private boolean withSchemaRegistry;
 
   // ZK Config
-  protected int zkPort;
   protected String zkConnect;
   protected EmbeddedZookeeper zookeeper;
-  protected ZkClient zkClient;
+  protected ZkUtils zkUtils;
   protected int zkConnectionTimeout = 6000;
   protected int zkSessionTimeout = 6000;
 
   // Kafka Config
-  protected List<Properties> configs = null;
+  protected List<KafkaConfig> configs = null;
   protected List<KafkaServer> servers = null;
   protected String brokerList = null;
 
@@ -74,7 +106,6 @@ public abstract class ClusterTestHarness {
   protected SchemaRegistryRestApplication schemaRegApp = null;
   protected Server schemaRegServer = null;
 
-  protected String bootstrapServers = null;
   protected Properties restProperties = null;
   protected KafkaRestConfig restConfig = null;
   protected TestKafkaRestApplication restApp = null;
@@ -86,37 +117,54 @@ public ClusterTestHarness() {
   }
 
   public ClusterTestHarness(int numBrokers, boolean withSchemaRegistry) {
-    // 1 port per broker + ZK + possibly SchemaReg + REST server
-    int numPorts = numBrokers + 3;
-    ports = new ArrayDeque<Integer>();
-    for (Object portObj : JavaConversions.asJavaList(TestUtils.choosePorts(numPorts))) {
-      ports.add((Integer) portObj);
-    }
-    zkPort = ports.remove();
-    zkConnect = String.format("localhost:%d", zkPort);
+    this.numBrokers = numBrokers;
+    this.withSchemaRegistry = withSchemaRegistry;
+
+    schemaRegProperties = new Properties();
+    restProperties = new Properties();
+  }
 
-    configs = new Vector<Properties>();
-    bootstrapServers = "";
+  public Properties overrideBrokerProperties(int i, Properties props) {
+    return props;
+  }
+
+  @Before
+  public void setUp() throws Exception {
+    zookeeper = new EmbeddedZookeeper();
+    zkConnect = String.format("127.0.0.1:%d", zookeeper.port());
+    zkUtils = ZkUtils.apply(
+        zkConnect, zkSessionTimeout, zkConnectionTimeout,
+        JaasUtils.isZkSecurityEnabled());
+
+    configs = new Vector<>();
+    servers = new Vector<>();
     for (int i = 0; i < numBrokers; i++) {
-      int port = ports.remove();
-      Properties props = TestUtils.createBrokerConfig(i, port, false);
-      // Turn auto creation *off*, unlike the default. This lets us test errors that should be
-      // generated when brokers are configured that way.
-      props.put("auto.create.topics.enable", "false");
+      final Option<java.io.File> noFile = scala.Option.apply(null);
+      final Option<SecurityProtocol> noInterBrokerSecurityProtocol = scala.Option.apply(null);
+      Properties props = TestUtils.createBrokerConfig(
+          i, zkConnect, false, false, TestUtils.RandomPort(), noInterBrokerSecurityProtocol,
+          noFile, true, false, TestUtils.RandomPort(), false, TestUtils.RandomPort(), false,
+          TestUtils.RandomPort());
+      props.setProperty("auto.create.topics.enable", "false");
       // We *must* override this to use the port we allocated (Kafka currently allocates one port
       // that it always uses for ZK
-      props.put("zookeeper.connect", this.zkConnect);
-      configs.add(props);
+      props.setProperty("zookeeper.connect", this.zkConnect);
 
-      if (bootstrapServers.length() > 0) {
-        bootstrapServers += ",";
-      }
-      bootstrapServers = bootstrapServers + "localhost:" + ((Integer) port).toString();
+      props = overrideBrokerProperties(i, props);
+
+      KafkaConfig config = new KafkaConfig(props);
+      configs.add(config);
+
+      KafkaServer server = TestUtils.createServer(config, SystemTime$.MODULE$);
+      servers.add(server);
     }
 
+    brokerList =
+        TestUtils.getBrokerListStrFromServers(JavaConversions.asScalaBuffer(servers),
+                                              SecurityProtocol.PLAINTEXT);
+
     if (withSchemaRegistry) {
-      schemaRegProperties = new Properties();
-      int schemaRegPort = ports.remove();
+      int schemaRegPort = choosePort();
       schemaRegProperties.put(SchemaRegistryConfig.PORT_CONFIG,
                               ((Integer) schemaRegPort).toString());
       schemaRegProperties.put(SchemaRegistryConfig.KAFKASTORE_CONNECTION_URL_CONFIG,
@@ -126,56 +174,23 @@ public ClusterTestHarness(int numBrokers, boolean withSchemaRegistry) {
       schemaRegProperties.put(SchemaRegistryConfig.COMPATIBILITY_CONFIG,
                               schemaRegCompatibility);
       schemaRegConnect = String.format("http://localhost:%d", schemaRegPort);
+
+      schemaRegApp =
+          new SchemaRegistryRestApplication(new SchemaRegistryConfig(schemaRegProperties));
+      schemaRegServer = schemaRegApp.createServer();
+      schemaRegServer.start();
     }
 
-    restProperties = new Properties();
-    int restPort = ports.remove();
+    int restPort = choosePort();
     restProperties.put(KafkaRestConfig.PORT_CONFIG, ((Integer) restPort).toString());
     restProperties.put(KafkaRestConfig.ZOOKEEPER_CONNECT_CONFIG, zkConnect);
     if (withSchemaRegistry) {
       restProperties.put(KafkaRestConfig.SCHEMA_REGISTRY_URL_CONFIG, schemaRegConnect);
     }
     restConnect = String.format("http://localhost:%d", restPort);
-  }
-
-  public Properties overrideBrokerProperties(int i, Properties props) {
-    return props;
-  }
-
-  @Before
-  public void setUp() throws Exception {
-    zookeeper = new EmbeddedZookeeper(zkConnect);
-    zkClient =
-        new ZkClient(zookeeper.connectString(), zkSessionTimeout, zkConnectionTimeout,
-                     ZKStringSerializer$.MODULE$);
-
-    if (configs == null || configs.size() <= 0) {
-      throw new RuntimeException("Must supply at least one server config.");
-    }
-
-    List<KafkaConfig> specializedConfigs = new ArrayList<KafkaConfig>();
-    for (int i = 0; i < configs.size(); i++) {
-      Properties refinedProps = (Properties) configs.get(i).clone();
-      refinedProps = overrideBrokerProperties(i, refinedProps);
-      specializedConfigs.add(new KafkaConfig(refinedProps));
-    }
-    brokerList = TestUtils.getBrokerListStrFromConfigs(
-        JavaConversions.asScalaIterable(specializedConfigs).toSeq());
-    servers = new Vector<KafkaServer>(specializedConfigs.size());
-    for (KafkaConfig config : specializedConfigs) {
-      KafkaServer server = TestUtils.createServer(config, SystemTime$.MODULE$);
-      servers.add(server);
-    }
-
-    if (schemaRegProperties != null) {
-      schemaRegApp =
-          new SchemaRegistryRestApplication(new SchemaRegistryConfig(schemaRegProperties));
-      schemaRegServer = schemaRegApp.createServer();
-      schemaRegServer.start();
-    }
 
     restConfig = new KafkaRestConfig(restProperties);
-    restApp = new TestKafkaRestApplication(restConfig, getZkClient(restConfig),
+    restApp = new TestKafkaRestApplication(restConfig, getZkUtils(restConfig),
                                            getMetadataObserver(restConfig),
                                            getProducerPool(restConfig),
                                            getConsumerManager(restConfig),
@@ -185,7 +200,7 @@ public void setUp() throws Exception {
     restServer.start();
   }
 
-  protected ZkClient getZkClient(KafkaRestConfig appConfig) {
+  protected ZkUtils getZkUtils(KafkaRestConfig appConfig) {
     return null;
   }
 
@@ -226,11 +241,11 @@ public void tearDown() throws Exception {
     }
     for (KafkaServer server : servers) {
       for (String logDir : JavaConversions.asJavaCollection(server.config().logDirs())) {
-        Utils.rm(logDir);
+        CoreUtils.rm(logDir);
       }
     }
 
-    zkClient.close();
+    zkUtils.close();
     zookeeper.shutdown();
   }
 
diff --git a/src/test/java/io/confluent/kafkarest/integration/ConsumerAvroTest.java b/src/test/java/io/confluent/kafkarest/integration/ConsumerAvroTest.java
index e21e78ae2b..786bc8159e 100644
--- a/src/test/java/io/confluent/kafkarest/integration/ConsumerAvroTest.java
+++ b/src/test/java/io/confluent/kafkarest/integration/ConsumerAvroTest.java
@@ -100,8 +100,8 @@ public void setUp() throws Exception {
     super.setUp();
     final int numPartitions = 3;
     final int replicationFactor = 1;
-    TestUtils.createTopic(zkClient, topicName, numPartitions, replicationFactor,
-                          JavaConversions.asScalaIterable(this.servers).toSeq(), new Properties());
+    TestUtils.createTopic(zkUtils, topicName, numPartitions, replicationFactor,
+                          JavaConversions.asScalaBuffer(this.servers), new Properties());
   }
 
   @Test
diff --git a/src/test/java/io/confluent/kafkarest/integration/ConsumerBinaryTest.java b/src/test/java/io/confluent/kafkarest/integration/ConsumerBinaryTest.java
index 2c0a1fd587..2cc42d42e4 100644
--- a/src/test/java/io/confluent/kafkarest/integration/ConsumerBinaryTest.java
+++ b/src/test/java/io/confluent/kafkarest/integration/ConsumerBinaryTest.java
@@ -77,8 +77,8 @@ public void setUp() throws Exception {
     super.setUp();
     final int numPartitions = 3;
     final int replicationFactor = 1;
-    TestUtils.createTopic(zkClient, topicName, numPartitions, replicationFactor,
-                          JavaConversions.asScalaIterable(this.servers).toSeq(), new Properties());
+    TestUtils.createTopic(zkUtils, topicName, numPartitions, replicationFactor,
+                          JavaConversions.asScalaBuffer(this.servers), new Properties());
   }
 
   @Test
diff --git a/src/test/java/io/confluent/kafkarest/integration/ConsumerJsonTest.java b/src/test/java/io/confluent/kafkarest/integration/ConsumerJsonTest.java
index 5caa90e564..e5d27a60a0 100644
--- a/src/test/java/io/confluent/kafkarest/integration/ConsumerJsonTest.java
+++ b/src/test/java/io/confluent/kafkarest/integration/ConsumerJsonTest.java
@@ -83,8 +83,8 @@ public void setUp() throws Exception {
     super.setUp();
     final int numPartitions = 3;
     final int replicationFactor = 1;
-    TestUtils.createTopic(zkClient, topicName, numPartitions, replicationFactor,
-        JavaConversions.asScalaIterable(this.servers).toSeq(), new Properties());
+    TestUtils.createTopic(zkUtils, topicName, numPartitions, replicationFactor,
+        JavaConversions.asScalaBuffer(this.servers), new Properties());
   }
 
   @Test
diff --git a/src/test/java/io/confluent/kafkarest/integration/ConsumerTimeoutTest.java b/src/test/java/io/confluent/kafkarest/integration/ConsumerTimeoutTest.java
index 71727cb57e..a35baa8a4b 100644
--- a/src/test/java/io/confluent/kafkarest/integration/ConsumerTimeoutTest.java
+++ b/src/test/java/io/confluent/kafkarest/integration/ConsumerTimeoutTest.java
@@ -48,8 +48,8 @@ public void setUp() throws Exception {
     super.setUp();
     final int numPartitions = 3;
     final int replicationFactor = 1;
-    TestUtils.createTopic(zkClient, topicName, numPartitions, replicationFactor,
-                          JavaConversions.asScalaIterable(this.servers).toSeq(), new Properties());
+    TestUtils.createTopic(zkUtils, topicName, numPartitions, replicationFactor,
+                          JavaConversions.asScalaBuffer(this.servers), new Properties());
   }
 
   @Test
diff --git a/src/test/java/io/confluent/kafkarest/integration/JsonProducerTest.java b/src/test/java/io/confluent/kafkarest/integration/JsonProducerTest.java
index 105cd9ba8f..97ef44122e 100644
--- a/src/test/java/io/confluent/kafkarest/integration/JsonProducerTest.java
+++ b/src/test/java/io/confluent/kafkarest/integration/JsonProducerTest.java
@@ -57,8 +57,8 @@ public void setUp() throws Exception {
     super.setUp();
     final int numPartitions = 3;
     final int replicationFactor = 1;
-    kafka.utils.TestUtils.createTopic(zkClient, topicName, numPartitions, replicationFactor,
-        JavaConversions.asScalaIterable(this.servers).toSeq(),
+    kafka.utils.TestUtils.createTopic(zkUtils, topicName, numPartitions, replicationFactor,
+        JavaConversions.asScalaBuffer(this.servers),
         new Properties());
   }
 
diff --git a/src/test/java/io/confluent/kafkarest/integration/MetadataAPITest.java b/src/test/java/io/confluent/kafkarest/integration/MetadataAPITest.java
index e5d7092df9..27ed1f1491 100644
--- a/src/test/java/io/confluent/kafkarest/integration/MetadataAPITest.java
+++ b/src/test/java/io/confluent/kafkarest/integration/MetadataAPITest.java
@@ -82,10 +82,10 @@ public MetadataAPITest() {
   @Override
   public void setUp() throws Exception {
     super.setUp();
-    TestUtils.createTopic(zkClient, topic1Name, topic1Partitions.size(), numReplicas,
-                          JavaConversions.asScalaIterable(this.servers).toSeq(), new Properties());
-    TestUtils.createTopic(zkClient, topic2Name, topic2Partitions.size(), numReplicas,
-                          JavaConversions.asScalaIterable(this.servers).toSeq(), topic2Configs);
+    TestUtils.createTopic(zkUtils, topic1Name, topic1Partitions.size(), numReplicas,
+                          JavaConversions.asScalaBuffer(this.servers), new Properties());
+    TestUtils.createTopic(zkUtils, topic2Name, topic2Partitions.size(), numReplicas,
+                          JavaConversions.asScalaBuffer(this.servers), topic2Configs);
   }
 
   @Test
diff --git a/src/test/java/io/confluent/kafkarest/integration/ProducerTest.java b/src/test/java/io/confluent/kafkarest/integration/ProducerTest.java
index 69e27b87f9..40116c65b7 100644
--- a/src/test/java/io/confluent/kafkarest/integration/ProducerTest.java
+++ b/src/test/java/io/confluent/kafkarest/integration/ProducerTest.java
@@ -15,8 +15,8 @@
  **/
 package io.confluent.kafkarest.integration;
 
-import org.I0Itec.zkclient.ZkClient;
 import org.apache.kafka.common.errors.RecordTooLargeException;
+import org.apache.kafka.common.security.JaasUtils;
 import org.junit.Before;
 import org.junit.Test;
 
@@ -24,36 +24,26 @@
 import java.util.List;
 import java.util.Properties;
 
-import javax.ws.rs.client.Entity;
-import javax.ws.rs.core.Response;
-
 import io.confluent.kafkarest.KafkaRestConfig;
 import io.confluent.kafkarest.ProducerPool;
 import io.confluent.kafkarest.RecordMetadataOrException;
-import io.confluent.kafkarest.TestUtils;
-import io.confluent.kafkarest.Versions;
 import io.confluent.kafkarest.entities.BinaryProduceRecord;
 import io.confluent.kafkarest.entities.BinaryTopicProduceRecord;
 import io.confluent.kafkarest.entities.EmbeddedFormat;
-import io.confluent.kafkarest.entities.Partition;
 import io.confluent.kafkarest.entities.PartitionOffset;
-import io.confluent.kafkarest.entities.PartitionProduceRequest;
-import io.confluent.kafkarest.entities.PartitionReplica;
 import io.confluent.kafkarest.entities.ProduceRecord;
-import io.confluent.kafkarest.entities.ProduceResponse;
 import kafka.serializer.Decoder;
 import kafka.serializer.DefaultDecoder;
-import kafka.utils.ZKStringSerializer$;
+import kafka.utils.ZkUtils;
 import scala.collection.JavaConversions;
 
-import static io.confluent.kafkarest.TestUtils.assertOKResponse;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertNotNull;
 import static org.junit.Assert.assertTrue;
 
 public class ProducerTest extends AbstractProducerTest {
 
-  private ZkClient testZkClient;
+  private ZkUtils testZkUtils;
 
   private static final String topicName = "topic1";
 
@@ -128,10 +118,11 @@ public class ProducerTest extends AbstractProducerTest {
   private boolean sawCallback;
 
   @Override
-  protected ZkClient getZkClient(KafkaRestConfig appConfig) {
-    testZkClient = new ZkClient(appConfig.getString(KafkaRestConfig.ZOOKEEPER_CONNECT_CONFIG),
-                                30000, 30000, ZKStringSerializer$.MODULE$);
-    return testZkClient;
+  protected ZkUtils getZkUtils(KafkaRestConfig appConfig) {
+    testZkUtils = ZkUtils.apply(
+        appConfig.getString(KafkaRestConfig.ZOOKEEPER_CONNECT_CONFIG), 30000, 30000,
+        JaasUtils.isZkSecurityEnabled());
+    return testZkUtils;
   }
 
   @Override
@@ -140,7 +131,7 @@ protected ProducerPool getProducerPool(KafkaRestConfig appConfig) {
     // Reduce the metadata fetch timeout so requests for topics that don't exist timeout much
     // faster than the default
     overrides.setProperty("metadata.fetch.timeout.ms", "5000");
-    return new ProducerPool(appConfig, testZkClient, overrides);
+    return new ProducerPool(appConfig, testZkUtils, overrides);
   }
 
   @Before
@@ -149,8 +140,8 @@ public void setUp() throws Exception {
     super.setUp();
     final int numPartitions = 3;
     final int replicationFactor = 1;
-    kafka.utils.TestUtils.createTopic(zkClient, topicName, numPartitions, replicationFactor,
-                                      JavaConversions.asScalaIterable(this.servers).toSeq(),
+    kafka.utils.TestUtils.createTopic(zkUtils, topicName, numPartitions, replicationFactor,
+                                      JavaConversions.asScalaBuffer(this.servers),
                                       new Properties());
   }
 
@@ -163,7 +154,7 @@ public void testProducerConfigOverrides() {
     overrides.setProperty("buffer.memory", "1");
     // Note separate ProducerPool since the override should only be for this test, so
     // getProducerPool doesn't work here
-    ProducerPool pool = new ProducerPool(this.restConfig, this.bootstrapServers, overrides);
+    ProducerPool pool = new ProducerPool(this.restConfig, this.brokerList, overrides);
 
     sawCallback = false;
     pool.produce(topicName, 0, EmbeddedFormat.BINARY, null,
diff --git a/src/test/java/io/confluent/kafkarest/integration/SimpleConsumerAvroTest.java b/src/test/java/io/confluent/kafkarest/integration/SimpleConsumerAvroTest.java
index 564d49b9ef..cc8680ce84 100644
--- a/src/test/java/io/confluent/kafkarest/integration/SimpleConsumerAvroTest.java
+++ b/src/test/java/io/confluent/kafkarest/integration/SimpleConsumerAvroTest.java
@@ -91,8 +91,8 @@ public void setUp() throws Exception {
     super.setUp();
     final int numPartitions = 1;
     final int replicationFactor = 1;
-    TestUtils.createTopic(zkClient, topicName, numPartitions, replicationFactor,
-                          JavaConversions.asScalaIterable(this.servers).toSeq(), new Properties());
+    TestUtils.createTopic(zkUtils, topicName, numPartitions, replicationFactor,
+                          JavaConversions.asScalaBuffer(this.servers), new Properties());
   }
 
   @Test
diff --git a/src/test/java/io/confluent/kafkarest/integration/SimpleConsumerBinaryTest.java b/src/test/java/io/confluent/kafkarest/integration/SimpleConsumerBinaryTest.java
index f5934e24b2..320f59a228 100644
--- a/src/test/java/io/confluent/kafkarest/integration/SimpleConsumerBinaryTest.java
+++ b/src/test/java/io/confluent/kafkarest/integration/SimpleConsumerBinaryTest.java
@@ -61,8 +61,8 @@ public void setUp() throws Exception {
     super.setUp();
     final int numPartitions = 1;
     final int replicationFactor = 1;
-    TestUtils.createTopic(zkClient, topicName, numPartitions, replicationFactor,
-                          JavaConversions.asScalaIterable(this.servers).toSeq(), new Properties());
+    TestUtils.createTopic(zkUtils, topicName, numPartitions, replicationFactor,
+                          JavaConversions.asScalaBuffer(this.servers), new Properties());
   }
 
   @Test
diff --git a/src/test/java/io/confluent/kafkarest/integration/SimpleConsumerJsonTest.java b/src/test/java/io/confluent/kafkarest/integration/SimpleConsumerJsonTest.java
index 38cbcb3449..03646cd4e5 100644
--- a/src/test/java/io/confluent/kafkarest/integration/SimpleConsumerJsonTest.java
+++ b/src/test/java/io/confluent/kafkarest/integration/SimpleConsumerJsonTest.java
@@ -86,8 +86,8 @@ public void setUp() throws Exception {
     super.setUp();
     final int numPartitions = 1;
     final int replicationFactor = 1;
-    TestUtils.createTopic(zkClient, topicName, numPartitions, replicationFactor,
-        JavaConversions.asScalaIterable(this.servers).toSeq(), new Properties());
+    TestUtils.createTopic(zkUtils, topicName, numPartitions, replicationFactor,
+        JavaConversions.asScalaBuffer(this.servers), new Properties());
   }
 
   @Test
diff --git a/src/test/java/io/confluent/kafkarest/integration/TestKafkaRestApplication.java b/src/test/java/io/confluent/kafkarest/integration/TestKafkaRestApplication.java
index 6d82ebd87e..f4b6230a45 100644
--- a/src/test/java/io/confluent/kafkarest/integration/TestKafkaRestApplication.java
+++ b/src/test/java/io/confluent/kafkarest/integration/TestKafkaRestApplication.java
@@ -17,7 +17,7 @@
 package io.confluent.kafkarest.integration;
 
 import io.confluent.kafkarest.*;
-import org.I0Itec.zkclient.ZkClient;
+import kafka.utils.ZkUtils;
 
 import javax.ws.rs.core.Configurable;
 
@@ -27,19 +27,19 @@
  */
 public class TestKafkaRestApplication extends KafkaRestApplication {
 
-  ZkClient zkClientInjected;
+  ZkUtils zkUtilsInjected;
   MetadataObserver mdObserverInjected;
   ProducerPool producerPoolInjected;
   ConsumerManager consumerManagerInjected;
   SimpleConsumerFactory simpleConsumerFactoryInjected;
   SimpleConsumerManager simpleConsumerManagerInjected;
 
-  public TestKafkaRestApplication(KafkaRestConfig config, ZkClient zkClient,
+  public TestKafkaRestApplication(KafkaRestConfig config, ZkUtils zkUtils,
                                   MetadataObserver mdObserver, ProducerPool producerPool,
                                   ConsumerManager consumerManager,
                                   SimpleConsumerFactory simpleConsumerFactory, SimpleConsumerManager simpleConsumerManager) {
     super(config);
-    zkClientInjected = zkClient;
+    zkUtilsInjected = zkUtils;
     mdObserverInjected = mdObserver;
     producerPoolInjected = producerPool;
     consumerManagerInjected = consumerManager;
@@ -49,7 +49,7 @@ public TestKafkaRestApplication(KafkaRestConfig config, ZkClient zkClient,
 
   @Override
   public void setupResources(Configurable<?> config, KafkaRestConfig appConfig) {
-    setupInjectedResources(config, appConfig, zkClientInjected, mdObserverInjected,
+    setupInjectedResources(config, appConfig, zkUtilsInjected, mdObserverInjected,
                            producerPoolInjected, consumerManagerInjected,
                            simpleConsumerFactoryInjected, simpleConsumerManagerInjected);
   }
diff --git a/src/test/java/io/confluent/kafkarest/mock/MockConsumerConnector.java b/src/test/java/io/confluent/kafkarest/mock/MockConsumerConnector.java
index 7503a345ac..6c7d283327 100644
--- a/src/test/java/io/confluent/kafkarest/mock/MockConsumerConnector.java
+++ b/src/test/java/io/confluent/kafkarest/mock/MockConsumerConnector.java
@@ -25,9 +25,12 @@
 import io.confluent.kafkarest.Time;
 import io.confluent.kafkarest.entities.ConsumerRecord;
 import kafka.common.MessageStreamsExistException;
+import kafka.common.OffsetAndMetadata;
+import kafka.common.TopicAndPartition;
 import kafka.consumer.KafkaStream;
 import kafka.consumer.TopicFilter;
 import kafka.javaapi.consumer.ConsumerConnector;
+import kafka.javaapi.consumer.ConsumerRebalanceListener;
 import kafka.serializer.Decoder;
 import kafka.serializer.DefaultDecoder;
 import kafka.utils.VerifiableProperties;
@@ -144,6 +147,17 @@ public void commitOffsets(boolean b) {
     throw new UnsupportedOperationException();
   }
 
+  @Override
+  public void commitOffsets(Map<TopicAndPartition, OffsetAndMetadata> offsetsToCommit,
+                            boolean retryOnFailure) {
+
+  }
+
+  @Override
+  public void setConsumerRebalanceListener(ConsumerRebalanceListener listener) {
+
+  }
+
   @Override
   public void shutdown() {
 
diff --git a/src/test/java/io/confluent/kafkarest/mock/MockConsumerQueue.java b/src/test/java/io/confluent/kafkarest/mock/MockConsumerQueue.java
index bd73d3b1a1..11b4614f20 100644
--- a/src/test/java/io/confluent/kafkarest/mock/MockConsumerQueue.java
+++ b/src/test/java/io/confluent/kafkarest/mock/MockConsumerQueue.java
@@ -120,8 +120,7 @@ public FetchedDataChunk poll(long timeout, TimeUnit unit) throws InterruptedExce
       }
 
       ByteBufferMessageSet msgSet = new ByteBufferMessageSet(
-          JavaConversions.asScalaIterable(Arrays.asList(new Message(c.getValue(), c.getKey())))
-              .toSeq()
+          JavaConversions.asScalaBuffer(Arrays.asList(new Message(c.getValue(), c.getKey())))
       );
       AtomicLong consumedOffset = new AtomicLong(0);
       AtomicLong fetchOffset = new AtomicLong(0);
